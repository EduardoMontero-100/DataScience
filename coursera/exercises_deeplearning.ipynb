{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 588.3 MB 6.0 kB/s eta 0:00:01   |▎                               | 5.6 MB 3.1 MB/s eta 0:03:08     |▋                               | 12.2 MB 288 kB/s eta 0:33:15     |████████▍                       | 153.7 MB 5.3 MB/s eta 0:01:23     |████████████████████            | 367.1 MB 3.9 MB/s eta 0:00:57     |██████████████████████▋         | 415.3 MB 4.4 MB/s eta 0:00:40     |████████████████████████        | 441.8 MB 2.7 MB/s eta 0:00:55     |█████████████████████████▍      | 467.0 MB 2.4 MB/s eta 0:00:51     |███████████████████████████▎    | 500.5 MB 2.9 MB/s eta 0:00:31     |██████████████████████████████▉ | 566.3 MB 1.8 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 183 kB/s eta 0:00:01     |██████▊                         | 1.3 MB 4.8 MB/s eta 0:00:01�████████████▋      | 4.8 MB 183 kB/s eta 0:00:07\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 212 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 326 kB/s eta 0:00:01    |██████                          | 2.7 MB 1.7 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 162 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[K     |████████████████████████████████| 439 kB 830 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 451 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/eduardo/.local/lib/python3.8/site-packages (from tensorflow) (1.22.3)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 658 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/eduardo/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.34.2)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 196 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 9.5 MB/s eta 0:00:0193 kB 4.0 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/eduardo/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/eduardo/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 434 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/eduardo/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.8.0)\n",
      "Installing collected packages: cachetools, pyasn1, pyasn1-modules, rsa, google-auth, grpcio, absl-py, protobuf, werkzeug, requests-oauthlib, google-auth-oauthlib, importlib-metadata, markdown, tensorboard-plugin-wit, tensorboard-data-server, tensorboard, flatbuffers, gast, google-pasta, libclang, typing-extensions, tensorflow-io-gcs-filesystem, keras, opt-einsum, h5py, tensorflow-estimator, wrapt, astunparse, termcolor, tensorflow\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.12.6 gast-0.4.0 google-auth-2.15.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 h5py-3.7.0 importlib-metadata-5.1.0 keras-2.11.0 libclang-14.0.6 markdown-3.4.1 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1 typing-extensions-4.4.0 werkzeug-2.2.2 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 25.6890\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 20.5044\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.4193\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.1995\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.6606\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6573\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0758\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8260\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8375\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0545\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.4334\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9397\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5464\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2322\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9803\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7775\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6135\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4800\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3706\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2804\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2052\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1420\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0883\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0421\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0020\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9667\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9352\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9069\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8811\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8574\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8353\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8147\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7952\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7767\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7590\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7421\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7258\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7101\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6948\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6801\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6657\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6517\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6381\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6247\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6118\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5991\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5867\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5745\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5627\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5511\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5397\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5286\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5177\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5071\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4966\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4864\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4764\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4666\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4571\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4477\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4385\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4295\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4206\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4120\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4035\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3952\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3871\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3792\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3714\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3637\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3563\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3490\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3418\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3348\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3279\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3212\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3146\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3081\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3018\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2956\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2895\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2836\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2777\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2720\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2664\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2610\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2556\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2504\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2452\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2402\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2352\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2304\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2257\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2210\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2165\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2121\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2077\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2034\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1993\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1952\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1912\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1872\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1834\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1796\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1759\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1723\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1688\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1653\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1619\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1586\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1553\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1521\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1490\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1459\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1430\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1400\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1371\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1343\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1316\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1289\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1262\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1236\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1211\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1186\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1162\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1138\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1114\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1091\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1069\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1047\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1026\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1005\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0984\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0964\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0944\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0924\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0905\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0887\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0869\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0851\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0833\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0816\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0799\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0783\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0767\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0751\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0736\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0721\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0706\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0691\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0677\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0663\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0650\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0636\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0623\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0610\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0598\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0586\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0574\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0562\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0550\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0539\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0528\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0517\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0506\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0496\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0486\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0476\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0466\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0456\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0447\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0438\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0429\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0420\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0412\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0403\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0395\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0387\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0379\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0371\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0363\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0356\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0349\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0341\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0334\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0328\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0321\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0314\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0308\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0301\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0295\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0289\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0283\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0277\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0272\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0266\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0261\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0255\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0250\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0245\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0240\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0235\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0230\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0221\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0216\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0212\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0207\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0203\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0199\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0195\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0191\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0187\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0183\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0179\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0176\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0172\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0169\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0165\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0162\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0158\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0155\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0149\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0143\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0140\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0137\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0134\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0131\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0129\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0123\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0121\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0118\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0116\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0114\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0111\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0109\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0107\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0105\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0102\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0100\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0098\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0096\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0094\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0092\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0090\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0089\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0087\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0085\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0083\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0082\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0080\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0078\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0077\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0075\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0073\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0071\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0069\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0068\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0066\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0065\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0062\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0061\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0060\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0058\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0057\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0056\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0055\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0054\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0053\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0052\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0051\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0050\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0049\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0048\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0047\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0046\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0045\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0044\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0043\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0042\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0041\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0040\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0039\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0039\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0037\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0036\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0036\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0035\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0034\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0033\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0033\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0032\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0031\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0031\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0029\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0028\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0027\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0025\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0024\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0024\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0023\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0022\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0022\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0021\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0021\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0019\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0019\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0018\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0017\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0017\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0016\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0015\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0015\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0014\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0013\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0013\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0012\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0012\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0011\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0011\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0011\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0010\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0010\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.8158e-04\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6142e-04\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4167e-04\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2233e-04\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.0338e-04\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.8483e-04\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6665e-04\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.4885e-04\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3141e-04\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1434e-04\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9761e-04\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.8122e-04\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.6518e-04\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.4946e-04\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.3407e-04\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.1899e-04\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0422e-04\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.8975e-04\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.7559e-04\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.6171e-04\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4811e-04\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.3480e-04\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2176e-04\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.0899e-04\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.9648e-04\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8423e-04\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.7223e-04\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.6047e-04\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.4896e-04\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3768e-04\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.2664e-04\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.1582e-04\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.0523e-04\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.9485e-04\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8469e-04\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7473e-04\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6498e-04\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.5543e-04\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.4607e-04\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3691e-04\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.2794e-04\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.1915e-04\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1054e-04\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.0210e-04\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9384e-04\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.8576e-04\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7783e-04\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7007e-04\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.6247e-04\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.5502e-04\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.4773e-04\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4059e-04\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.3359e-04\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2674e-04\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2003e-04\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1346e-04\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0702e-04\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.0071e-04\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.9453e-04\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8848e-04\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8256e-04\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7675e-04\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7107e-04\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6550e-04\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.6005e-04\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5471e-04\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4947e-04\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4435e-04\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3933e-04\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3442e-04\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2960e-04\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2488e-04\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2026e-04\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1574e-04\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1131e-04\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0697e-04\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0272e-04\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9855e-04\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9447e-04\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9048e-04\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8657e-04\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8273e-04\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7898e-04\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7531e-04\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7170e-04\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6818e-04\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6472e-04\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6134e-04\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5802e-04\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5478e-04\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5160e-04\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4849e-04\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4544e-04\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4245e-04\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3952e-04\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3666e-04\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3385e-04\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3110e-04\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2841e-04\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.2577e-04\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2319e-04\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2066e-04\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1818e-04\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1575e-04\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1337e-04\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1104e-04\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0876e-04\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0653e-04\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0434e-04\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0220e-04\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0010e-04\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.8043e-05\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6029e-05\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4056e-05\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2124e-05\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.0232e-05\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.8379e-05\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.6563e-05\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.4785e-05\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3043e-05\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.1338e-05\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9667e-05\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.8030e-05\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.6428e-05\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.4858e-05\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.3320e-05\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1814e-05\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0339e-05\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.8895e-05\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.7479e-05\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.6093e-05\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.4735e-05\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.3406e-05\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2103e-05\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.0828e-05\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9578e-05\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.8354e-05\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.7156e-05\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5981e-05\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4831e-05\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3705e-05\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.2602e-05\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.1522e-05\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.0463e-05\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.9426e-05\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.8411e-05\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[[18.9797]]\n"
     ]
    }
   ],
   "source": [
    "# Una red neuronal con un layer (una capa) y una neurona\n",
    "# Creamos el modelo usando keras sequencial class. que nos permite crear una red neuronal como\n",
    "# una secuencia de layers\n",
    "# usamos Dense to build a simple network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units = 1, input_shape = [1])\n",
    "])\n",
    "\n",
    "# when we COMPILE a network we need to specify to functions: a loss function and an optimizer\n",
    "# it will repeat the guessing process until usando la funcion de perdida y tratando de \n",
    "# minimizar el error. eso se va a repetir en base a la cantidad de epochs  \n",
    "# compiling the model:\n",
    "### sgd: stochastic gradient descent\n",
    "### loss: mean_squared_error\n",
    "model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\n",
    "\n",
    "## Declare model inputs and outputs for training\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype = float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype= float)\n",
    "\n",
    "## Training a model\n",
    "model.fit(xs, ys, epochs=500)\n",
    "\n",
    "## Make a prediction\n",
    "print(model.predict([10.0]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fashion-nmist documentacion](https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'module'>\n"
     ]
    }
   ],
   "source": [
    "## Introduction to computer vision\n",
    "# working with the dataset mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "fashion_mnist_data = fashion_mnist\n",
    "print(type(fashion_mnist_data))\n",
    "(train_images, train_labels),(test_images, test_labels)= fashion_mnist_data.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))\n",
    "print(type(train_labels))\n",
    "print(type(test_images))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de clases en el dataset: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Cantidad de clases en el dataset: {}'.format(np.unique(train_labels).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n",
      "Image Array: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2eb5f7ad90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Index of the images\n",
    "index = 0\n",
    "\n",
    "# Seteamos el número de caracteres por fila cuando lo printeamos\n",
    "np.set_printoptions(linewidth=320)\n",
    "\n",
    "# Print the label and image\n",
    "print(f'Label: {train_labels[index]}')\n",
    "print(f'Image Array: {train_images[index]}')\n",
    "\n",
    "## Visualize image\n",
    "plt.imshow(train_images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizamos los valores entre 0 y 1\n",
    "train_images = train_images / np.max(train_images)\n",
    "test_images = test_images /np.max(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the classification model\n",
    "model = keras.models.Sequential(\n",
    "                    [\n",
    "                        keras.layers.Flatten(),\n",
    "                        keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "                        keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "                    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to softmax function: [[1. 3. 4. 2.]]\n",
      "output of softmax function: [[0.0320586  0.23688282 0.64391426 0.08714432]]\n",
      "sum of outputs 1.0\n",
      "class with the highest probability: 2\n"
     ]
    }
   ],
   "source": [
    "## Sequential: That defines a sequence of layers in the neural network\n",
    "## Flatten: Toma las imagenes que estan en matrices de 28*28 y lo transforma en 1-dimensinal array\n",
    "## Dense: Add a layer network\n",
    "## Relu: Pasa solo valores de 0 o mayores a 0 a la siguiente capa\n",
    "##if x > 0: \n",
    "##  return x\n",
    "##else: \n",
    "##  return 0\n",
    "## Softmax: Softmax takes a list of values and scales these so the sum of all elements \n",
    "# will be equal to 1. When applied to model outputs, you can think of the scaled values as \n",
    "# the probability for that class. Examples:\n",
    "\n",
    "# Declaramos algunos inputs de ejemplo y los transformamos a tensores\n",
    "inputs = np.array([[1.0, 3.0, 4.0, 2.0]])\n",
    "inputs = tf.convert_to_tensor(inputs)\n",
    "print(f'input to softmax function: {inputs.numpy()}')\n",
    "\n",
    "# Feed the input to softmax activation function\n",
    "outputs = keras.activations.softmax(inputs)\n",
    "print(f'output of softmax function: {outputs.numpy()}')\n",
    "\n",
    "# Get the sum of all values after softmax\n",
    "sum = tf.reduce_sum(outputs)\n",
    "print(f'sum of outputs {sum}')\n",
    "\n",
    "# Get the index of the highest value\n",
    "prediction = np.argmax(outputs)\n",
    "print(f'class with the highest probability: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# Creamos una red.\n",
    "# Primero tenemos la capa inicial: input_data\n",
    "# Luego tenemos la capa oculta\n",
    "# Luego tenemos la capa final (tiene una neurona por cada clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:18:06.381647: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5006 - accuracy: 0.8234\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3769 - accuracy: 0.8639\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3372 - accuracy: 0.8770\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3115 - accuracy: 0.8870\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2943 - accuracy: 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f0c2077f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construimos el modelo\n",
    "# primero lo compilamos:\n",
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    ")\n",
    "# hacemos el fit del modelo\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3446 - accuracy: 0.8731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34460148215293884, 0.8730999827384949]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate the model on unseen data\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using callbacks to control training\n",
    "### tf.keras.callbacks.Callback base class.\n",
    "###You can create a callback by defining a class that inherits the tf.keras.callbacks.Callback base class. \n",
    "# From there, you can define available methods to set where the callback will be executed. For instance below, \n",
    "# you will use the on_epoch_end() method to check the loss at each training epoch.\n",
    "\n",
    "class myCallback(keras.callbacks.Callback): # Clase que hereda de tf.keras.callbacks.Callback \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Halts the training after reach 60 percent accuracy\n",
    "\n",
    "        Args:\n",
    "            epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "            logs (dict) - metric results from training epochs\n",
    "        \"\"\"\n",
    "\n",
    "        # Check accuracy\n",
    "        if(logs.get('loss')<0.4):\n",
    "\n",
    "            # Stop if threshold is met\n",
    "            print('\\n Loss is lower than 0.4 so cancelling training!')\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Instantiate Class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino nuevamente el modelo\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(512, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:48:06.501873: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 11s 5ms/step - loss: 0.4755 - accuracy: 0.8309\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.8685\n",
      " Loss is lower than 0.4 so cancelling training!\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3600 - accuracy: 0.8685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2eb5ba87f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with a call back\n",
    "model.fit(train_images, train_labels, epochs=10, callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
