{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 588.3 MB 6.0 kB/s eta 0:00:01   |▎                               | 5.6 MB 3.1 MB/s eta 0:03:08     |▋                               | 12.2 MB 288 kB/s eta 0:33:15     |████████▍                       | 153.7 MB 5.3 MB/s eta 0:01:23     |████████████████████            | 367.1 MB 3.9 MB/s eta 0:00:57     |██████████████████████▋         | 415.3 MB 4.4 MB/s eta 0:00:40     |████████████████████████        | 441.8 MB 2.7 MB/s eta 0:00:55     |█████████████████████████▍      | 467.0 MB 2.4 MB/s eta 0:00:51     |███████████████████████████▎    | 500.5 MB 2.9 MB/s eta 0:00:31     |██████████████████████████████▉ | 566.3 MB 1.8 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 183 kB/s eta 0:00:01     |██████▊                         | 1.3 MB 4.8 MB/s eta 0:00:01�████████████▋      | 4.8 MB 183 kB/s eta 0:00:07\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 212 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 326 kB/s eta 0:00:01    |██████                          | 2.7 MB 1.7 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 162 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[K     |████████████████████████████████| 439 kB 830 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 451 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/eduardo/.local/lib/python3.8/site-packages (from tensorflow) (1.22.3)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 658 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/eduardo/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.34.2)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 196 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 9.5 MB/s eta 0:00:0193 kB 4.0 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/eduardo/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/eduardo/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 434 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/eduardo/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.8.0)\n",
      "Installing collected packages: cachetools, pyasn1, pyasn1-modules, rsa, google-auth, grpcio, absl-py, protobuf, werkzeug, requests-oauthlib, google-auth-oauthlib, importlib-metadata, markdown, tensorboard-plugin-wit, tensorboard-data-server, tensorboard, flatbuffers, gast, google-pasta, libclang, typing-extensions, tensorflow-io-gcs-filesystem, keras, opt-einsum, h5py, tensorflow-estimator, wrapt, astunparse, termcolor, tensorflow\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.12.6 gast-0.4.0 google-auth-2.15.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 h5py-3.7.0 importlib-metadata-5.1.0 keras-2.11.0 libclang-14.0.6 markdown-3.4.1 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1 typing-extensions-4.4.0 werkzeug-2.2.2 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 25.6890\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 20.5044\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.4193\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.1995\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.6606\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6573\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0758\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8260\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8375\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0545\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.4334\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9397\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5464\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2322\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9803\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7775\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6135\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4800\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3706\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2804\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2052\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1420\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0883\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0421\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0020\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9667\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9352\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9069\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8811\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8574\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8353\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8147\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7952\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7767\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7590\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7421\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7258\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7101\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6948\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6801\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6657\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6517\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6381\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6247\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6118\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5991\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5867\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5745\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5627\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5511\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5397\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5286\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5177\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5071\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4966\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4864\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4764\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4666\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4571\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4477\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4385\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4295\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4206\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4120\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4035\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3952\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3871\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3792\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3714\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3637\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3563\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3490\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3418\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3348\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3279\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3212\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3146\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3081\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3018\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2956\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2895\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2836\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2777\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2720\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2664\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2610\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2556\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2504\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2452\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2402\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2352\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2304\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2257\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2210\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2165\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2121\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2077\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2034\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1993\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1952\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1912\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1872\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1834\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1796\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1759\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1723\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1688\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1653\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1619\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1586\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1553\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1521\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1490\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1459\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1430\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1400\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1371\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1343\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1316\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1289\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1262\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1236\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1211\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1186\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1162\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1138\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1114\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1091\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1069\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1047\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1026\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1005\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0984\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0964\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0944\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0924\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0905\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0887\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0869\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0851\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0833\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0816\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0799\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0783\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0767\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0751\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0736\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0721\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0706\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0691\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0677\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0663\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0650\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0636\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0623\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0610\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0598\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0586\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0574\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0562\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0550\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0539\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0528\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0517\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0506\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0496\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0486\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0476\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0466\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0456\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0447\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0438\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0429\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0420\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0412\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0403\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0395\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0387\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0379\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0371\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0363\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0356\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0349\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0341\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0334\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0328\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0321\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0314\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0308\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0301\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0295\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0289\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0283\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0277\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0272\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0266\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0261\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0255\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0250\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0245\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0240\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0235\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0230\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0225\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0221\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0216\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0212\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0207\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0203\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0199\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0195\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0191\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0187\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0183\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0179\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0176\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0172\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0169\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0165\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0162\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0158\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0155\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0152\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0149\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0143\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0140\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0137\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0134\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0131\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0129\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0123\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0121\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0118\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0116\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0114\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0111\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0109\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0107\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0105\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0102\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0100\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0098\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0096\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0094\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0092\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0090\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0089\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0087\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0085\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0083\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0082\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0080\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0078\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0077\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0075\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0073\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0071\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0069\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0068\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0066\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0065\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0062\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0061\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0060\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0058\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0057\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0056\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0055\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0054\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0053\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0052\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0051\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0050\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0049\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0048\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0047\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0046\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0045\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0044\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0043\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0042\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0041\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0040\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0039\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0039\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0037\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0036\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0036\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0035\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0034\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0033\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0033\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0032\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0031\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0031\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0030\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0029\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0029\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0028\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0028\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0027\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0025\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0024\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0024\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0023\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0022\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0022\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0021\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0021\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0019\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0019\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0018\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0018\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0017\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0017\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0016\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0015\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0015\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0014\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0013\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0013\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0012\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0012\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0011\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0011\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0011\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0010\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0010\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.8158e-04\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6142e-04\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4167e-04\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2233e-04\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.0338e-04\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.8483e-04\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6665e-04\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.4885e-04\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3141e-04\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1434e-04\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9761e-04\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.8122e-04\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.6518e-04\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.4946e-04\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.3407e-04\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.1899e-04\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0422e-04\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.8975e-04\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.7559e-04\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.6171e-04\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4811e-04\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.3480e-04\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2176e-04\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.0899e-04\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.9648e-04\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8423e-04\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.7223e-04\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.6047e-04\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.4896e-04\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3768e-04\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.2664e-04\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.1582e-04\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.0523e-04\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.9485e-04\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8469e-04\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7473e-04\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6498e-04\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.5543e-04\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.4607e-04\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3691e-04\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.2794e-04\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.1915e-04\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1054e-04\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.0210e-04\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9384e-04\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.8576e-04\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7783e-04\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7007e-04\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.6247e-04\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.5502e-04\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.4773e-04\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.4059e-04\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.3359e-04\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2674e-04\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2003e-04\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1346e-04\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0702e-04\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.0071e-04\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.9453e-04\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8848e-04\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8256e-04\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7675e-04\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7107e-04\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6550e-04\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.6005e-04\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5471e-04\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4947e-04\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4435e-04\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3933e-04\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3442e-04\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2960e-04\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2488e-04\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2026e-04\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1574e-04\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1131e-04\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0697e-04\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0272e-04\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9855e-04\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9447e-04\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9048e-04\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8657e-04\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8273e-04\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7898e-04\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7531e-04\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7170e-04\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6818e-04\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6472e-04\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6134e-04\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5802e-04\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5478e-04\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5160e-04\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4849e-04\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4544e-04\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4245e-04\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3952e-04\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3666e-04\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3385e-04\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3110e-04\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2841e-04\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.2577e-04\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2319e-04\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2066e-04\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1818e-04\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1575e-04\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1337e-04\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1104e-04\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0876e-04\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0653e-04\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0434e-04\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0220e-04\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0010e-04\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.8043e-05\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6029e-05\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4056e-05\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2124e-05\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.0232e-05\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.8379e-05\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.6563e-05\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.4785e-05\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3043e-05\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.1338e-05\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9667e-05\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.8030e-05\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.6428e-05\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.4858e-05\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.3320e-05\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1814e-05\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0339e-05\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.8895e-05\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.7479e-05\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.6093e-05\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.4735e-05\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.3406e-05\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2103e-05\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.0828e-05\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9578e-05\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.8354e-05\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.7156e-05\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5981e-05\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4831e-05\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3705e-05\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.2602e-05\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.1522e-05\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.0463e-05\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.9426e-05\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.8411e-05\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[[18.9797]]\n"
     ]
    }
   ],
   "source": [
    "# Una red neuronal con un layer (una capa) y una neurona\n",
    "# Creamos el modelo usando keras sequencial class. que nos permite crear una red neuronal como\n",
    "# una secuencia de layers\n",
    "# usamos Dense to build a simple network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units = 1, input_shape = [1])\n",
    "])\n",
    "\n",
    "# when we COMPILE a network we need to specify to functions: a loss function and an optimizer\n",
    "# it will repeat the guessing process until usando la funcion de perdida y tratando de \n",
    "# minimizar el error. eso se va a repetir en base a la cantidad de epochs  \n",
    "# compiling the model:\n",
    "### sgd: stochastic gradient descent\n",
    "### loss: mean_squared_error\n",
    "model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\n",
    "\n",
    "## Declare model inputs and outputs for training\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype = float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype= float)\n",
    "\n",
    "## Training a model\n",
    "model.fit(xs, ys, epochs=500)\n",
    "\n",
    "## Make a prediction\n",
    "print(model.predict([10.0]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fashion-nmist documentacion](https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'module'>\n"
     ]
    }
   ],
   "source": [
    "## Introduction to computer vision\n",
    "# working with the dataset mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "fashion_mnist_data = fashion_mnist\n",
    "print(type(fashion_mnist_data))\n",
    "(train_images, train_labels),(test_images, test_labels)= fashion_mnist_data.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))\n",
    "print(type(train_labels))\n",
    "print(type(test_images))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de clases en el dataset: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Cantidad de clases en el dataset: {}'.format(np.unique(train_labels).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n",
      "Image Array: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2eb5f7ad90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUFElEQVR4nO3da2yc1ZkH8P8z4/ElzjiJk+CE4BIuoZDCEqhJuIlSKDREVQOli4gQC1K0QbvQbbt8ANGuyn5ZIbSA0LLbXQNZwqpQtSoIiiIKmEsWKGlMSHPdEEgcEuPYTkxsx/HYc3n2g1+oCT7Pa+adGzn/n2R5PM+cmeMZ//3OzJlzjqgqiOj4Fyt3B4ioNBh2Ik8w7ESeYNiJPMGwE3miqpQ3Vi01Wov6Ut4kkVdSGMKojshEtUhhF5GlAB4GEAfwmKreZ12+FvVYIldGuUkiMqzXNmct76fxIhIH8O8ArgGwEMAKEVmY7/URUXFFec2+GMAHqrpbVUcB/BrA8sJ0i4gKLUrY5wHYN+7n/cF5nyMiq0SkXUTa0xiJcHNEFEXR341X1VZVbVHVlgRqin1zROQQJeydAJrH/XxScB4RVaAoYd8AYIGInCIi1QBuBPB8YbpFRIWW99CbqmZE5A4Af8DY0NtqVd1WsJ4RUUFFGmdX1bUA1haoL0RURPy4LJEnGHYiTzDsRJ5g2Ik8wbATeYJhJ/IEw07kCYadyBMMO5EnGHYiTzDsRJ5g2Ik8wbATeaKkS0lTGciEqwr/RcSNPeMzG836J989w1lreOqdSLcd9rtJVcJZ0/RotNuOKuxxseT5mPHITuQJhp3IEww7kScYdiJPMOxEnmDYiTzBsBN5guPsxzmJx826ZjJmPbbI3qtzx21T7fbD7lpiaLHZtmo4Z9YTL7Wb9Uhj6WFj+CH3K8Q+jkbpm1QZsTUeTh7ZiTzBsBN5gmEn8gTDTuQJhp3IEww7kScYdiJPcJz9OGeOySJ8nH3fd6eb9Zsu+l+z/lbvqc7a3po5ZlutM8uo+s5FZv2M/+h01jIdH9lXHjJnPOx+CxOfMcNdzGbNttmBAXfR6HaksItIB4BBAFkAGVVtiXJ9RFQ8hTiyf1tVDxbgeoioiPiancgTUcOuAF4SkXdFZNVEFxCRVSLSLiLtaYxEvDkiylfUp/GXqmqniJwA4GUR+T9VXTf+AqraCqAVABqkMdrqhkSUt0hHdlXtDL73AHgWgD2NiYjKJu+wi0i9iCQ/PQ3gagBbC9UxIiqsKE/jmwA8K2PzfqsAPKWqLxakV1QwuVQqUvvR846Y9R9Os+eU18bSztobMXu+euerzWY9+1d23/Y+mHTWcu9dbLadudUe6254r8usH7xsnlnv/ab7FW1TyHL6M1750FmTPnek8w67qu4GcG6+7YmotDj0RuQJhp3IEww7kScYdiJPMOxEnhCNuGXvl9EgjbpErizZ7XnDWvY45PE9csOFZv2an79u1s+q/disD+ZqnbVRjfYBzkd2fsusD+2e5qzFRkO2TA4pZ5vspaA1bR9HZ2x0/+51y7vNtvLobGdtc9vDONK3b8Le88hO5AmGncgTDDuRJxh2Ik8w7ESeYNiJPMGwE3mC4+yVIGR74EhCHt+z37X/3/9ghj2FNUzcWNt4SKvNtoez9ZFuuzfjnuKaDhnjf2yXPQX2iDGGDwCxjP2YXvXt95y16xs3mG3vP+0cZ229tmFA+zjOTuQzhp3IEww7kScYdiJPMOxEnmDYiTzBsBN5gls2V4ISftbhWLuOnGDWDzVMNesHMtPN+sy4e7nnZGzYbDs/Ye8X2pt1j6MDQDzhXqp6VONm23/+xu/NeuqshFlPiL0U9cXGOgB/vf1vzLb12G3WXXhkJ/IEw07kCYadyBMMO5EnGHYiTzDsRJ5g2Ik8wXF2z82usbc9rhX3lssAUC0Zs/5xeoaztmv462bb9wfszwAsbdpm1tPGWLo1zx4IHyc/MfGJWU+pPQ5v3auXNNnj6JvMqlvokV1EVotIj4hsHXdeo4i8LCK7gu/uR5SIKsJknsY/AWDpMefdDaBNVRcAaAt+JqIKFhp2VV0HoO+Ys5cDWBOcXgPg2sJ2i4gKLd/X7E2q2hWcPgCgyXVBEVkFYBUA1GJKnjdHRFFFfjdex1asdL7boaqtqtqiqi0J1ES9OSLKU75h7xaRuQAQfO8pXJeIqBjyDfvzAG4JTt8C4LnCdIeIiiX0NbuIPA3gcgCzRGQ/gF8AuA/Ab0RkJYC9AG4oZiePeyHrxkvcnnutGfdYd3yGPSr6relbzHpvtsGsH87a78NMjx911gYz7r3bAaBv2L7uM2u6zPrGo/OdtdnV9ji51W8A6BidZdYX1Bww6/d3u/dPaK499v3wz8tceZmzpuv/6KyFhl1VVzhK3O2B6CuEH5cl8gTDTuQJhp3IEww7kScYdiJPcIprJQhZSlqq7IfJGnrbt/Iss+0VU+wlk99OzTPrs6sGzbo1zXRuTb/ZNtmUMuthw36NVe7pu4PZOrPtlNiIWQ/7vc+vtpfB/ukr5ztrybMPmW0bEsYx2hjF5ZGdyBMMO5EnGHYiTzDsRJ5g2Ik8wbATeYJhJ/IEx9krgCSqzXouZY83W2ZtGTXrB7P2ksfTY/ZUz+qQJZetrZEvbtxjtu0NGQvfOHyKWU/G3VtCz47Z4+TNCXuse0uq2ayvHTrdrK/83ivO2tOtV5ltq19821kTdT9ePLITeYJhJ/IEw07kCYadyBMMO5EnGHYiTzDsRJ74ao2zG0suS5U9XizxkP9rMbueSxnzm3P2WHMYTdtj4VE8/F+PmPV9melm/UDaroctuZw1Jli/MzzNbFsbs7eLnl01YNYHcvY4vWUwZy9zbc3TB8L7ftfMXc7aM/3fMdvmi0d2Ik8w7ESeYNiJPMGwE3mCYSfyBMNO5AmGncgTFTXOHmV99LCxarWHPctqePlis77vWnsc/6bz/uSsHcgkzbbvGdsaA8A0Y044ANSHrK+eUvfnHz4etbeTDhurttaFB4ATjHH4rNrHuc603bcwYZ8/2J8x1rT/vj3XfvqTeXUp/MguIqtFpEdEto47714R6RSRTcHXsvxunohKZTJP458AsHSC8x9S1UXB19rCdouICi007Kq6DkBfCfpCREUU5Q26O0Rkc/A03/kCR0RWiUi7iLSnYb++I6LiyTfsvwRwGoBFALoAPOC6oKq2qmqLqrYkUJPnzRFRVHmFXVW7VTWrqjkAjwKw304morLLK+wiMnfcj9cB2Oq6LBFVhtBxdhF5GsDlAGaJyH4AvwBwuYgsAqAAOgDcVojOWOPoUVXNnWPW06c0mfW+s9x7gR+dY2yKDWDRsh1m/dam/zbrvdkGs54QY3/29Eyz7XlTOsz6q/0LzfrBqqlm3Rqnv7jePacbAA7n7P3XT6z6xKzf9cEPnbWmKfZY9mMn2wNMac2Z9Z1p+yVrf849H/4fFr5mtn0Ws826S2jYVXXFBGc/ntetEVHZ8OOyRJ5g2Ik8wbATeYJhJ/IEw07kiYqa4jpyzQVm/YSf7XbWFjXsN9surHvTrKdy9lLU1nTL7cPzzLZHc/aWzLtG7WHB/ow9BBUX9zBQz6g9xfWBPfayxW2L/9Os//zjieZI/UWsTp21Q1l72O76qfZS0YD9mN32tXXO2qnVPWbbF4bmmvWPQ6bANiX6zfr8RK+z9oPk+2bbfIfeeGQn8gTDTuQJhp3IEww7kScYdiJPMOxEnmDYiTxR2nF2sZeLXvIvG8zmVya3OWtH1Z5SGDaOHjZuaplWZS8bPJK27+aetD2FNcwZNQectesaNplt1z2yxKxfmvqRWf/wCnt6btuweypnb8b+vW/cc4VZ3/hRs1m/cP4eZ+2cZKfZNuyzDcl4yqxb044BYCjn/nt9J2V//iBfPLITeYJhJ/IEw07kCYadyBMMO5EnGHYiTzDsRJ4QVfd840Krm9Osp938j8566+3/ZrZ/qu9CZ6251t6O7uTqg2Z9Ztze/teSjNljrl9P2GOuLwydZNZfP3ymWf9mssNZS4i93fPlUz4w67f+9E6znqm1l9EemO8+nmTq7b+9hnMPmfUfnf6qWa82fvfDWXscPex+C9uSOYy1BkEyZm+T/cCy65y1P3Y8gf7hrgkfFB7ZiTzBsBN5gmEn8gTDTuQJhp3IEww7kScYdiJPlHQ+eywNTOl2jy++MLDIbH9qnXut7YNpe330Pxw5x6yfVGdv/2ttPXy6MZ8cADalppv1F3u/YdZPrLPXT+9OT3PWDqXrzbZHjXnVAPD4Qw+a9Qe67XXnr2vc6KydW22Pox/O2cei7SHr7Q/map21lNrrG/SHjMMnjb8HAEirHa24seXz9Jg9hj9wjnsb7my3+3ZDj+wi0iwir4nIdhHZJiI/Ds5vFJGXRWRX8D3/1R+IqOgm8zQ+A+BOVV0I4EIAt4vIQgB3A2hT1QUA2oKfiahChYZdVbtUdWNwehDADgDzACwHsCa42BoA1xapj0RUAF/qDToRmQ/gPADrATSpaldQOgCgydFmlYi0i0h7ZmQoSl+JKIJJh11EpgL4HYCfqOrn3jHSsdk0E85qUNVWVW1R1ZaqGvvNIiIqnkmFXUQSGAv6r1T1meDsbhGZG9TnArC3xSSisgodehMRAfA4gB2qOn4c5nkAtwC4L/j+XNh1xUdzSO4bcdZzak+XfPWge6pnU+2g2XZRcp9Z33nUHsbZMnyis7ax6mtm27q4e7tnAJhWbU+Rra9y32cAMCvh/t1PqbH/B1vTQAFgQ8r+3f5u9utm/aOMe5Dm90NnmG23H3Xf5wAwI2QJ7y0D7vZHM/Y22iNZOxqpjD2UO63GfkwvaNzrrO2EvV1077nGtOG33O0mM85+CYCbAWwRkU3BefdgLOS/EZGVAPYCuGES10VEZRIadlV9E4DrkHtlYbtDRMXCj8sSeYJhJ/IEw07kCYadyBMMO5EnSrtl85FhxN54z1n+7UuXmM3/aflvnbU3QpZbfuGAPS46MGpP9Zw9xf1R3wZjnBsAGhP2x4TDtnyuDdn+95OM+5OJIzF7KmfWOdAy5sCIe/osALyVW2DW0zn3ls0jRg0I/3xC3+gss35iXb+zNphxT38FgI7BRrN+sN/eVjk1xY7Wm9nTnLWlc9xbkwNAXY/7MYsZfyo8shN5gmEn8gTDTuQJhp3IEww7kScYdiJPMOxEnijpls0N0qhLJP+Jcv03ubdsPvXvd5ptF0/fY9Y3Dtjztj8yxl3TIUseJ2LuZYMBYEpi1KzXhow3V8fdc9JjEy8g9JlcyDh7fdzuW9hc+4Yq97zuZNye8x0ztjWejLjxu/+pf36k606G/N4Ztf8mLpr2obO2es/FZttpy9zbbK/XNgxoH7dsJvIZw07kCYadyBMMO5EnGHYiTzDsRJ5g2Ik8Ufpx9vjV7gvk7DXMoxi6folZX3LPBruedI+LnlndbbZNwB4vrg0ZT66P2WPhKeMxDPtv/uZws1nPhlzDq5+cZdbTxnhz99EGs23C+PzAZFj7EAxnQrZsHrbnu8djdm5Sr9tz7Wdud392omat/bdo4Tg7ETHsRL5g2Ik8wbATeYJhJ/IEw07kCYadyBOh4+wi0gzgSQBNABRAq6o+LCL3AvhbAL3BRe9R1bXWdUWdz16p5AJ7TfrhOXVmveaQPTd68GS7fcOH7nXpYyP2mvO5P+8w6/TVYo2zT2aTiAyAO1V1o4gkAbwrIi8HtYdU9V8L1VEiKp7J7M/eBaArOD0oIjsAzCt2x4iosL7Ua3YRmQ/gPADrg7PuEJHNIrJaRGY42qwSkXYRaU/DfrpKRMUz6bCLyFQAvwPwE1UdAPBLAKcBWISxI/8DE7VT1VZVbVHVlgTs/dSIqHgmFXYRSWAs6L9S1WcAQFW7VTWrqjkAjwJYXLxuElFUoWEXEQHwOIAdqvrguPPnjrvYdQC2Fr57RFQok3k3/hIANwPYIiKbgvPuAbBCRBZhbDiuA8BtRejfV4Ju2GLW7cmS4Rrezr9ttMWY6XgymXfj3wQmXFzcHFMnosrCT9AReYJhJ/IEw07kCYadyBMMO5EnGHYiTzDsRJ5g2Ik8wbATeYJhJ/IEw07kCYadyBMMO5EnGHYiT5R0y2YR6QWwd9xZswAcLFkHvpxK7Vul9gtg3/JVyL6drKqzJyqUNOxfuHGRdlVtKVsHDJXat0rtF8C+5atUfePTeCJPMOxEnih32FvLfPuWSu1bpfYLYN/yVZK+lfU1OxGVTrmP7ERUIgw7kSfKEnYRWSoiO0XkAxG5uxx9cBGRDhHZIiKbRKS9zH1ZLSI9IrJ13HmNIvKyiOwKvk+4x16Z+naviHQG990mEVlWpr41i8hrIrJdRLaJyI+D88t63xn9Ksn9VvLX7CISB/A+gKsA7AewAcAKVd1e0o44iEgHgBZVLfsHMETkMgBHADypqmcH590PoE9V7wv+Uc5Q1bsqpG/3AjhS7m28g92K5o7fZhzAtQBuRRnvO6NfN6AE91s5juyLAXygqrtVdRTArwEsL0M/Kp6qrgPQd8zZywGsCU6vwdgfS8k5+lYRVLVLVTcGpwcBfLrNeFnvO6NfJVGOsM8DsG/cz/tRWfu9K4CXRORdEVlV7s5MoElVu4LTBwA0lbMzEwjdxruUjtlmvGLuu3y2P4+Kb9B90aWqej6AawDcHjxdrUg69hqsksZOJ7WNd6lMsM34Z8p53+W7/XlU5Qh7J4DmcT+fFJxXEVS1M/jeA+BZVN5W1N2f7qAbfO8pc38+U0nbeE+0zTgq4L4r5/bn5Qj7BgALROQUEakGcCOA58vQjy8QkfrgjROISD2Aq1F5W1E/D+CW4PQtAJ4rY18+p1K28XZtM44y33dl3/5cVUv+BWAZxt6R/xDAz8rRB0e/TgXw5+BrW7n7BuBpjD2tS2PsvY2VAGYCaAOwC8ArABorqG//A2ALgM0YC9bcMvXtUow9Rd8MYFPwtazc953Rr5Lcb/y4LJEn+AYdkScYdiJPMOxEnmDYiTzBsBN5gmEn8gTDTuSJ/wcK8iUIg3ozJAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Index of the images\n",
    "index = 0\n",
    "\n",
    "# Seteamos el número de caracteres por fila cuando lo printeamos\n",
    "np.set_printoptions(linewidth=320)\n",
    "\n",
    "# Print the label and image\n",
    "print(f'Label: {train_labels[index]}')\n",
    "print(f'Image Array: {train_images[index]}')\n",
    "\n",
    "## Visualize image\n",
    "plt.imshow(train_images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizamos los valores entre 0 y 1\n",
    "train_images = train_images / np.max(train_images)\n",
    "test_images = test_images /np.max(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the classification model\n",
    "model = keras.models.Sequential(\n",
    "                    [\n",
    "                        keras.layers.Flatten(),\n",
    "                        keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "                        keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "                    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to softmax function: [[1. 3. 4. 2.]]\n",
      "output of softmax function: [[0.0320586  0.23688282 0.64391426 0.08714432]]\n",
      "sum of outputs 1.0\n",
      "class with the highest probability: 2\n"
     ]
    }
   ],
   "source": [
    "## Sequential: That defines a sequence of layers in the neural network\n",
    "## Flatten: Toma las imagenes que estan en matrices de 28*28 y lo transforma en 1-dimensinal array\n",
    "## Dense: Add a layer network\n",
    "## Relu: Pasa solo valores de 0 o mayores a 0 a la siguiente capa\n",
    "##if x > 0: \n",
    "##  return x\n",
    "##else: \n",
    "##  return 0\n",
    "## Softmax: Softmax takes a list of values and scales these so the sum of all elements \n",
    "# will be equal to 1. When applied to model outputs, you can think of the scaled values as \n",
    "# the probability for that class. Examples:\n",
    "\n",
    "# Declaramos algunos inputs de ejemplo y los transformamos a tensores\n",
    "inputs = np.array([[1.0, 3.0, 4.0, 2.0]])\n",
    "inputs = tf.convert_to_tensor(inputs)\n",
    "print(f'input to softmax function: {inputs.numpy()}')\n",
    "\n",
    "# Feed the input to softmax activation function\n",
    "outputs = keras.activations.softmax(inputs)\n",
    "print(f'output of softmax function: {outputs.numpy()}')\n",
    "\n",
    "# Get the sum of all values after softmax\n",
    "sum = tf.reduce_sum(outputs)\n",
    "print(f'sum of outputs {sum}')\n",
    "\n",
    "# Get the index of the highest value\n",
    "prediction = np.argmax(outputs)\n",
    "print(f'class with the highest probability: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# Creamos una red.\n",
    "# Primero tenemos la capa inicial: input_data\n",
    "# Luego tenemos la capa oculta\n",
    "# Luego tenemos la capa final (tiene una neurona por cada clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:18:06.381647: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5006 - accuracy: 0.8234\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3769 - accuracy: 0.8639\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3372 - accuracy: 0.8770\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3115 - accuracy: 0.8870\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2943 - accuracy: 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f0c2077f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construimos el modelo\n",
    "# primero lo compilamos:\n",
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    ")\n",
    "# hacemos el fit del modelo\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3446 - accuracy: 0.8731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34460148215293884, 0.8730999827384949]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate the model on unseen data\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using callbacks to control training\n",
    "### tf.keras.callbacks.Callback base class.\n",
    "###You can create a callback by defining a class that inherits the tf.keras.callbacks.Callback base class. \n",
    "# From there, you can define available methods to set where the callback will be executed. For instance below, \n",
    "# you will use the on_epoch_end() method to check the loss at each training epoch.\n",
    "\n",
    "class myCallback(keras.callbacks.Callback): # Clase que hereda de tf.keras.callbacks.Callback \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Halts the training after reach 60 percent accuracy\n",
    "\n",
    "        Args:\n",
    "            epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "            logs (dict) - metric results from training epochs\n",
    "        \"\"\"\n",
    "\n",
    "        # Check accuracy\n",
    "        if(logs.get('loss')<0.4):\n",
    "\n",
    "            # Stop if threshold is met\n",
    "            print('\\n Loss is lower than 0.4 so cancelling training!')\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Instantiate Class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino nuevamente el modelo\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(512, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:48:06.501873: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 11s 5ms/step - loss: 0.4755 - accuracy: 0.8309\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.8685\n",
      " Loss is lower than 0.4 so cancelling training!\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3600 - accuracy: 0.8685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2eb5ba87f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with a call back\n",
    "model.fit(train_images, train_labels, epochs=10, callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convolutional Neural Networks\n",
    "### Looks for the most importance features\n",
    "### La convolución es una especie de transformación que permite dar mayor enfasís o peso a ciertas variables.\n",
    "### filtrando y comprimiendo: Para esto, definimos convolutional and pooling layers. At the top of the neurons.\n",
    "### El objetivo de las convoluciones si simplemente reducir los features y quedarnos con las más importantes.\n",
    "### Converting Deep Neural Network to Convolutional Neural Network by adding convolutional networks at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Loading fashion_mnist dataset\n",
    "fmnist = fashion_mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "# Normalize the pixes values\n",
    "train_images = train_images/255.0\n",
    "test_images = test_images/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 13:54:09.833926: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4978 - accuracy: 0.8261\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3749 - accuracy: 0.8653\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3355 - accuracy: 0.8781\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3112 - accuracy: 0.8862\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2923 - accuracy: 0.8921\n",
      "Model Evaluation:\n",
      "\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3758 - accuracy: 0.8674\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "        keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setup training parameters\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print('Model Training:\\n')\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Evaluate on the test set\n",
    "print('Model Evaluation:\\n')\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 13, 13, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " Train model\n",
      "1875/1875 [==============================] - 57s 30ms/step - loss: 0.4381 - accuracy: 0.8407\n",
      "\n",
      " Model evaluation\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3755 - accuracy: 0.8634\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# Para añadir convoluciones se añaden convolutions and max pooling\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Conv2D(64, (3,3), activation = 'relu', input_shape = (28,28,1)),\n",
    "        keras.layers.MaxPool2D(2,2),\n",
    "        keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n",
    "        keras.layers.MaxPool2D(2,2),\n",
    "        # Add the same layers as before\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "        keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "    ]\n",
    ") \n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Use some setting\n",
    "model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train model\n",
    "print('\\n Train model')\n",
    "model.fit(train_images, train_labels)\n",
    "\n",
    "# Evaluate on test set\n",
    "print('\\n Model evaluation')\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models\n",
    "\n",
    "f, axarr = plt.subplots(3,4)\n",
    "\n",
    "FIRST_IMAGE = 0\n",
    "SECOND_IMAGE = 23\n",
    "THIRD_IMAGE = 28\n",
    "CONVOLUTION_NUMBER = 1\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "for x in range(0,4):\n",
    "    f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1,28,28,1))[x]\n",
    "    axarr[0,x].imshow(f1[0,:,:, CONVOLUTION_NUMBER], cmap = 'inferno')\n",
    "    axarr[0,x].grid(False)\n",
    "\n",
    "    f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1,28,28,1))[x]\n",
    "    axarr[1,x].imshow(f2[0,:,:, CONVOLUTION_NUMBER], cmap = 'inferno')\n",
    "    axarr[1,x].grid(False)\n",
    "\n",
    "    f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1,28,28,1))[x]\n",
    "    axarr[2,x].imshow(f2[0,:,:, CONVOLUTION_NUMBER], cmap = 'inferno')\n",
    "    axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
