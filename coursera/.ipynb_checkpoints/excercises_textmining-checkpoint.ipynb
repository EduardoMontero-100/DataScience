{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitives constructs in text\n",
    "text1 = 'Ethics are built right into the ideals and objectives of the United Nations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primitive tokenization\n",
    "text2 = text1.split(' ')\n",
    "len(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics',\n",
       " 'are',\n",
       " 'built',\n",
       " 'right',\n",
       " 'into',\n",
       " 'the',\n",
       " 'ideals',\n",
       " 'and',\n",
       " 'objectives',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'Nations']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics',\n",
       " 'built',\n",
       " 'right',\n",
       " 'into',\n",
       " 'ideals',\n",
       " 'objectives',\n",
       " 'United',\n",
       " 'Nations']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking for long words\n",
    "[w for w in text2 if len(w)>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics', 'United', 'Nations']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking for capitalize words\n",
    "[w for w in text2 if w.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics', 'ideals', 'objectives', 'Nations']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words that ends with s\n",
    "[w for w in text2 if w.endswith('s')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = 'To be or not to be'\n",
    "text4 = text3.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To', 'be', 'or', 'not', 'to', 'be']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'To', 'be', 'not', 'or', 'to'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([w.lower() for w in text4 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be', 'not', 'or', 'to'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([w.lower() for w in text4 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## s.startswith(t)\n",
    "## s.endswith(t)\n",
    "## t in s\n",
    "## s.isupper(), s.islower(), s.istitle()\n",
    "## s.isalpha(), s.isdigit(), s.isalnum()\n",
    "## s.split()\n",
    "## s.splitlines()\n",
    "## s.join(list)\n",
    "## s.strip(), s.rstrip()\n",
    "## s.find(t), s.rfind(t)\n",
    "## s.replace(u,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = 'ouagadougou'\n",
    "text6 = text5.split('ou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'agad', 'g', '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ouagadougou'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ou'.join(text6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty separator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/eduardo/Datos/repo_personal/DataScience/coursera/excercises_textmining.ipynb Celda 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/eduardo/Datos/repo_personal/DataScience/coursera/excercises_textmining.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text5\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mValueError\u001b[0m: empty separator"
     ]
    }
   ],
   "source": [
    "text5.split('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'u', 'a', 'g', 'a', 'd', 'o', 'u', 'g', 'o', 'u']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the characters\n",
    "list(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'u', 'a', 'g', 'a', 'd', 'o', 'u', 'g', 'o', 'u']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the characters\n",
    "[c for c in text5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " 'A',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumped',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog.',\n",
       " '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cleaning text\n",
    "text8 = '   A quick brown fox jumped over the lazy dog. '\n",
    "text8.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A quick brown fox jumped over the lazy dog.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text9 = text8.strip()\n",
    "text9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text9.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text9.find('o') # where is the first o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A quick brOwn fOx jumped Over the lazy dOg.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text9.replace('o', 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Esta es una prueba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Esta es otra prueba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Esto lo hago para probar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     textos\n",
       "0        Esta es una prueba\n",
       "1       Esta es otra prueba\n",
       "2  Esto lo hago para probar"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'textos': ['Esta es una prueba', \n",
    "                             'Esta es otra prueba', \n",
    "                             'Esto lo hago para probar']})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/eduardo/Datos/repo_personal/DataScience/coursera/excercises_textmining.ipynb Celda 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/eduardo/Datos/repo_personal/DataScience/coursera/excercises_textmining.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mtextos\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mfind(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "df['textos'].find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    3\n",
       "2    9\n",
       "Name: textos, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textos'].str.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('UNDHR.txt', 'r') # filename, mode\n",
    "## f.readline(), f.read(), f.read(n)\n",
    "## for line in f: doSomething(line)\n",
    "## f.seek(n) reset the reading position\n",
    "## f.write(message)  write a message to the file (if it is opened in the appropriate mode)\n",
    "## f.close()\n",
    "## f.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Universal Declaration of Human Rights - English\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading files line by line\n",
    "f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the full line\n",
    "f.seek(0) # resets the reading\n",
    "text12 = f.read() # it will read the whole file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12534"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text13 = text12.splitlines()\n",
    "len(text13) # number of lines. Which ends with a \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Universal Declaration of Human Rights - English'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text13[0] # first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Universal Declaration of Human Rights - English'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('UNDHR.txt', 'r')\n",
    "text14 = f.readline()\n",
    "## removing the last newline character\n",
    "text14.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expresions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text10 = 'Ethics are built right into the ideals and objectives of the United Nations #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr @UN @UN_Women '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics',\n",
       " 'are',\n",
       " 'built',\n",
       " 'right',\n",
       " 'into',\n",
       " 'the',\n",
       " 'ideals',\n",
       " 'and',\n",
       " 'objectives',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'Nations',\n",
       " '#UNSG',\n",
       " '@',\n",
       " 'NY',\n",
       " 'Society',\n",
       " 'for',\n",
       " 'Ethical',\n",
       " 'Culture',\n",
       " 'bit.ly/2guVelr',\n",
       " '@UN',\n",
       " '@UN_Women',\n",
       " '']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = text10.split(' ')\n",
    "text11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#UNSG']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## finding hashtags\n",
    "[w for w in text11 if w.startswith('#')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@', '@UN', '@UN_Women']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## callouts\n",
    "[w for w in text11 if w.startswith('@')]\n",
    "## here is where regular expresion appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@UN', '@UN_Women']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "[w for w in text11 if re.search('@[A-Za-z0-9_]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regular expresions:\n",
    "## . ==> matches with a single character\n",
    "## ^ ==> it indicates the start of the string\n",
    "## $ ==> the end of the string\n",
    "## [] ==> matches one the set of characters within []\n",
    "## [a-z] ==> matches one of the range characters a,b,c,..,z\n",
    "## [^abc] ==> matches a character that is not a,b, or c\n",
    "## a|b ==> matches either a or b, where a and b are strings\n",
    "\n",
    "## \\b matches word boundry\n",
    "## \\d any digit, equivalent to [0-9]\n",
    "## \\D any non-digit, equivalent to [^0-9]\n",
    "## \\s any whitespace\n",
    "## \\S any non-whitespace\n",
    "## \\w alphanumeric character\n",
    "## \\W any non-alphanumeric character\n",
    "\n",
    "## * matches 0 or more times\n",
    "## + matches 1 or more times\n",
    "## ? matches 0 or more times\n",
    "## {n} exactly n repetitions, n>=0\n",
    "## {n,} at least n repetitions\n",
    "## {,n} at most n repetitions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@UN', '@UN_Women']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in text11 if re.search('@\\w+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'u', 'a', 'a', 'o', 'u', 'o', 'u']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## finding specific characters\n",
    "text12 = 'ouagadougou'\n",
    "re.findall(r'[aeiou]', text12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'd', 'g']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[^aeiou]', text12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regular expresions for dates\n",
    "## 23/10/2022\n",
    "\n",
    "dateStr = '23-10-200\\n23-10-2002\\n23/10/2002\\n23/10/02\\n10/23/2002\\n23 Oct 2002\\n23 October 2002\\nOct 23, 2002\\n October 23, 2002\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23-10-200', '23-10-2002', '23/10/2002', '23/10/02', '10/23/2002']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', dateStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oct']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\d{2} (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \\d{4}', dateStr)\n",
    "## como esta en parentesis no hace ninguna otra busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 Oct 2002', '23 October 2002']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\d{2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{4}', dateStr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text data in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday:The doctors appointment is at 2:45pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday: The dentists appointment is at 11:30 am.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday: At 7:00pm, there is a basketball game!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday: Be back home by 11:15 pm at the latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday: Take the train at 08:10 am, arrive at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0       Monday:The doctors appointment is at 2:45pm.\n",
       "1  Tuesday: The dentists appointment is at 11:30 am.\n",
       "2  Wednesday: At 7:00pm, there is a basketball game!\n",
       "3   Thursday: Be back home by 11:15 pm at the latest\n",
       "4  Friday: Take the train at 08:10 am, arrive at ..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_sentences = ['Monday:The doctors appointment is at 2:45pm.',\n",
    "                  'Tuesday: The dentists appointment is at 11:30 am.',\n",
    "                  'Wednesday: At 7:00pm, there is a basketball game!',\n",
    "                  'Thursday: Be back home by 11:15 pm at the latest',\n",
    "                  'Friday: Take the train at 08:10 am, arrive at 09:00am.'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(time_sentences, columns = ['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    44\n",
       "1    49\n",
       "2    49\n",
       "3    48\n",
       "4    54\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     6\n",
       "1     8\n",
       "2     8\n",
       "3    10\n",
       "4    10\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: text, dtype: bool"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.contains('appointment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    4\n",
       "2    3\n",
       "3    4\n",
       "4    8\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.count(r'\\d')\n",
    "# cuenta las veces que aparece un digito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [2, 4, 5]\n",
       "1                [1, 1, 3, 0]\n",
       "2                   [7, 0, 0]\n",
       "3                [1, 1, 1, 5]\n",
       "4    [0, 8, 1, 0, 0, 9, 0, 0]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.findall(r'\\d')\n",
    "# muestra los digitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [(2, 45)]\n",
       "1              [(11, 30)]\n",
       "2               [(7, 00)]\n",
       "3              [(11, 15)]\n",
       "4    [(08, 10), (09, 00)]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.findall(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7614/2023952851.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'].str.replace(r'\\w+day\\b', '???')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0            ???:The doctors appointment is at 2:45pm.\n",
       "1        ???: The dentists appointment is at 11:30 am.\n",
       "2          ???: At 7:00pm, there is a basketball game!\n",
       "3          ???: Be back home by 11:15 pm at the latest\n",
       "4    ???: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.replace(r'\\w+day\\b', '???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7614/1376844824.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'].str.replace(r'(\\w+day\\b)', lambda x: x.groups()[0][:3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0            Mon:The doctors appointment is at 2:45pm.\n",
       "1        Tue: The dentists appointment is at 11:30 am.\n",
       "2          Wed: At 7:00pm, there is a basketball game!\n",
       "3          Thu: Be back home by 11:15 pm at the latest\n",
       "4    Fri: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.replace(r'(\\w+day\\b)', lambda x: x.groups()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0   2  45\n",
       "1  11  30\n",
       "2   7  00\n",
       "3  11  15\n",
       "4  08  10"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.extract(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2:45pm</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7:00pm</td>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>09:00am</td>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0   1   2   3\n",
       "  match                     \n",
       "0 0       2:45pm   2  45  pm\n",
       "2 0       7:00pm   7  00  pm\n",
       "4 0      09:00am  09  00  am"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.extractall(r'((\\d?\\d):(\\d\\d)?([ap]m))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2:45pm</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7:00pm</td>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>09:00am</td>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time hour minute period\n",
       "  match                            \n",
       "0 0       2:45pm    2     45     pm\n",
       "2 0       7:00pm    7     00     pm\n",
       "4 0      09:00am   09     00     am"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.extractall(r'(?P<time>(?P<hour>\\d?\\d):(?P<minute>\\d\\d)?(?P<period>[ap]m))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Natural Language Processing task with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Wall Street Journal>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent7) ## number of tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text7)) ## unique number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cotton',\n",
       " '*T*-61',\n",
       " 'reality',\n",
       " 'geography',\n",
       " '*T*-226',\n",
       " 'subjecting',\n",
       " 'turf',\n",
       " 'weakening',\n",
       " 'anticipates',\n",
       " '41.60']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(text7))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequency distribution\n",
    "dist = FreqDist(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 4885, 'the': 4045, '.': 3828, 'of': 2319, 'to': 2164, 'a': 1878, 'in': 1572, 'and': 1511, '*-1': 1123, '0': 1099, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dist.keys()\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist['four'] ## esta palabra aparece 20 veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion',\n",
       " 'company',\n",
       " 'president',\n",
       " 'because',\n",
       " 'market',\n",
       " 'million',\n",
       " 'shares',\n",
       " 'trading',\n",
       " 'program']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## frequent words if len of this word is > 5 and dist >100\n",
    "freqw = [w for w in vocab if len(w)>5 and dist[w]>100]\n",
    "freqw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization and stemming\n",
    "input1 = \"List listed lists listing listings\"\n",
    "# lower case \n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming (looking for the root form of the word)\n",
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'rights',\n",
       " 'of']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization is a slight variant of stemming\n",
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "udhr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['univers',\n",
       " 'declar',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'preambl',\n",
       " 'wherea',\n",
       " 'recognit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inher',\n",
       " 'digniti',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalien',\n",
       " 'right',\n",
       " 'of',\n",
       " 'all',\n",
       " 'member',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'famili',\n",
       " 'is',\n",
       " 'the',\n",
       " 'foundat',\n",
       " 'of',\n",
       " 'freedom',\n",
       " ',',\n",
       " 'justic',\n",
       " 'and',\n",
       " 'peac',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'wherea',\n",
       " 'disregard',\n",
       " 'and',\n",
       " 'contempt',\n",
       " 'for',\n",
       " 'human',\n",
       " 'right',\n",
       " 'have',\n",
       " 'result',\n",
       " 'in',\n",
       " 'barbar',\n",
       " 'act',\n",
       " 'which',\n",
       " 'have',\n",
       " 'outrag',\n",
       " 'the',\n",
       " 'conscienc',\n",
       " 'of',\n",
       " 'mankind',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'advent',\n",
       " 'of',\n",
       " 'a',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'human',\n",
       " 'be',\n",
       " 'shall',\n",
       " 'enjoy',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'speech',\n",
       " 'and',\n",
       " 'belief',\n",
       " 'and',\n",
       " 'freedom',\n",
       " 'from',\n",
       " 'fear',\n",
       " 'and',\n",
       " 'want',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'proclaim',\n",
       " 'as',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'aspir',\n",
       " 'of',\n",
       " 'the',\n",
       " 'common',\n",
       " 'peopl',\n",
       " ',',\n",
       " 'wherea',\n",
       " 'it',\n",
       " 'is',\n",
       " 'essenti',\n",
       " ',',\n",
       " 'if',\n",
       " 'man',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'be',\n",
       " 'compel',\n",
       " 'to',\n",
       " 'have',\n",
       " 'recours',\n",
       " ',',\n",
       " 'as',\n",
       " 'a',\n",
       " 'last',\n",
       " 'resort',\n",
       " ',',\n",
       " 'to',\n",
       " 'rebellion',\n",
       " 'against',\n",
       " 'tyranni',\n",
       " 'and',\n",
       " 'oppress',\n",
       " ',',\n",
       " 'that',\n",
       " 'human',\n",
       " 'right',\n",
       " 'should',\n",
       " 'be',\n",
       " 'protect',\n",
       " 'by',\n",
       " 'the',\n",
       " 'rule',\n",
       " 'of',\n",
       " 'law',\n",
       " ',',\n",
       " 'wherea',\n",
       " 'it',\n",
       " 'is',\n",
       " 'essenti',\n",
       " 'to',\n",
       " 'promot',\n",
       " 'the',\n",
       " 'develop',\n",
       " 'of',\n",
       " 'friendli',\n",
       " 'relat',\n",
       " 'between',\n",
       " 'nation',\n",
       " ',',\n",
       " 'wherea',\n",
       " 'the',\n",
       " 'peopl',\n",
       " 'of',\n",
       " 'the',\n",
       " 'unit',\n",
       " 'nation',\n",
       " 'have',\n",
       " 'in',\n",
       " 'the',\n",
       " 'charter',\n",
       " 'reaffirm',\n",
       " 'their',\n",
       " 'faith',\n",
       " 'in',\n",
       " 'fundament',\n",
       " 'human',\n",
       " 'right',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'digniti',\n",
       " 'and',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'person',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'right',\n",
       " 'of',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'and',\n",
       " 'have',\n",
       " 'determin',\n",
       " 'to',\n",
       " 'promot',\n",
       " 'social',\n",
       " 'progress',\n",
       " 'and',\n",
       " 'better',\n",
       " 'standard',\n",
       " 'of',\n",
       " 'life',\n",
       " 'in',\n",
       " 'larger',\n",
       " 'freedom',\n",
       " ',',\n",
       " 'wherea',\n",
       " 'member',\n",
       " 'state',\n",
       " 'have',\n",
       " 'pledg',\n",
       " 'themselv',\n",
       " 'to',\n",
       " 'achiev',\n",
       " ',',\n",
       " 'in',\n",
       " 'cooper',\n",
       " 'with',\n",
       " 'the',\n",
       " 'unit',\n",
       " 'nation',\n",
       " ',',\n",
       " 'the',\n",
       " 'promot',\n",
       " 'of',\n",
       " 'univers',\n",
       " 'respect',\n",
       " 'for',\n",
       " 'and',\n",
       " 'observ',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'and',\n",
       " 'fundament',\n",
       " 'freedom',\n",
       " ',',\n",
       " 'wherea',\n",
       " 'a',\n",
       " 'common',\n",
       " 'understand',\n",
       " 'of',\n",
       " 'these',\n",
       " 'right',\n",
       " 'and',\n",
       " 'freedom',\n",
       " 'is',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'import',\n",
       " 'for',\n",
       " 'the',\n",
       " 'full',\n",
       " 'realiz',\n",
       " 'of',\n",
       " 'thi',\n",
       " 'pledg',\n",
       " ',',\n",
       " 'now',\n",
       " ',',\n",
       " 'therefor',\n",
       " ',',\n",
       " 'the',\n",
       " 'gener',\n",
       " 'assembl',\n",
       " ',',\n",
       " 'proclaim',\n",
       " 'thi',\n",
       " 'univers',\n",
       " 'declar',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'as',\n",
       " 'a',\n",
       " 'common',\n",
       " 'standard',\n",
       " 'of',\n",
       " 'achiev',\n",
       " 'for',\n",
       " 'all',\n",
       " 'peopl',\n",
       " 'and',\n",
       " 'all',\n",
       " 'nation',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'end',\n",
       " 'that',\n",
       " 'everi',\n",
       " 'individu',\n",
       " 'and',\n",
       " 'everi',\n",
       " 'organ',\n",
       " 'of',\n",
       " 'societi',\n",
       " ',',\n",
       " 'keep',\n",
       " 'thi',\n",
       " 'declar',\n",
       " 'constantli',\n",
       " 'in',\n",
       " 'mind',\n",
       " ',',\n",
       " 'shall',\n",
       " 'strive',\n",
       " 'by',\n",
       " 'teach',\n",
       " 'and',\n",
       " 'educ',\n",
       " 'to',\n",
       " 'promot',\n",
       " 'respect',\n",
       " 'for',\n",
       " 'these',\n",
       " 'right',\n",
       " 'and',\n",
       " 'freedom',\n",
       " 'and',\n",
       " 'by',\n",
       " 'progress',\n",
       " 'measur',\n",
       " ',',\n",
       " 'nation',\n",
       " 'and',\n",
       " 'intern',\n",
       " ',',\n",
       " 'to',\n",
       " 'secur',\n",
       " 'their',\n",
       " 'univers',\n",
       " 'and',\n",
       " 'effect',\n",
       " 'recognit',\n",
       " 'and',\n",
       " 'observ',\n",
       " ',',\n",
       " 'both',\n",
       " 'among',\n",
       " 'the',\n",
       " 'peopl',\n",
       " 'of',\n",
       " 'member',\n",
       " 'state',\n",
       " 'themselv',\n",
       " 'and',\n",
       " 'among',\n",
       " 'the',\n",
       " 'peopl',\n",
       " 'of',\n",
       " 'territori',\n",
       " 'under',\n",
       " 'their',\n",
       " 'jurisdict',\n",
       " '.',\n",
       " 'articl',\n",
       " '1',\n",
       " 'all',\n",
       " 'human',\n",
       " 'be',\n",
       " 'are',\n",
       " 'born',\n",
       " 'free',\n",
       " 'and',\n",
       " 'equal',\n",
       " 'in',\n",
       " 'digniti',\n",
       " 'and',\n",
       " 'right',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'endow',\n",
       " 'with',\n",
       " 'reason',\n",
       " 'and',\n",
       " 'conscienc',\n",
       " 'and',\n",
       " 'should',\n",
       " 'act',\n",
       " 'toward',\n",
       " 'one',\n",
       " 'anoth',\n",
       " 'in',\n",
       " 'a',\n",
       " 'spirit',\n",
       " 'of',\n",
       " 'brotherhood',\n",
       " '.',\n",
       " 'articl',\n",
       " '2',\n",
       " 'everyon',\n",
       " 'is',\n",
       " 'entitl',\n",
       " 'to',\n",
       " 'all',\n",
       " 'the',\n",
       " 'right',\n",
       " 'and',\n",
       " 'freedom',\n",
       " 'set',\n",
       " 'forth',\n",
       " 'in',\n",
       " 'thi',\n",
       " 'declar',\n",
       " ',',\n",
       " 'without',\n",
       " 'distinct',\n",
       " 'of',\n",
       " 'ani',\n",
       " 'kind',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'race',\n",
       " ',',\n",
       " 'colour',\n",
       " ',',\n",
       " 'sex',\n",
       " ',',\n",
       " 'languag',\n",
       " ',',\n",
       " 'religion',\n",
       " ',',\n",
       " 'polit',\n",
       " 'or',\n",
       " 'other',\n",
       " 'opinion',\n",
       " ',',\n",
       " 'nation',\n",
       " 'or',\n",
       " 'social',\n",
       " 'origin',\n",
       " ',',\n",
       " 'properti',\n",
       " ',',\n",
       " 'birth',\n",
       " 'or',\n",
       " 'other',\n",
       " 'statu',\n",
       " '.',\n",
       " 'furthermor',\n",
       " ',',\n",
       " 'no',\n",
       " 'distinct',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'made',\n",
       " 'on',\n",
       " 'the',\n",
       " 'basi',\n",
       " 'of',\n",
       " 'the',\n",
       " 'polit',\n",
       " ',',\n",
       " 'jurisdict',\n",
       " 'or',\n",
       " 'intern',\n",
       " 'statu',\n",
       " 'of',\n",
       " 'the',\n",
       " 'countri',\n",
       " 'or',\n",
       " 'territori',\n",
       " 'to',\n",
       " 'which',\n",
       " 'a',\n",
       " 'person',\n",
       " 'belong',\n",
       " ',',\n",
       " 'whether',\n",
       " 'it',\n",
       " 'be',\n",
       " 'independ',\n",
       " ',',\n",
       " 'trust',\n",
       " ',',\n",
       " 'non',\n",
       " '-',\n",
       " 'self',\n",
       " '-',\n",
       " 'govern',\n",
       " 'or',\n",
       " 'under',\n",
       " 'ani',\n",
       " 'other',\n",
       " 'limit',\n",
       " 'of',\n",
       " 'sovereignti',\n",
       " '.',\n",
       " 'articl',\n",
       " '3',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'life',\n",
       " ',',\n",
       " 'liberti',\n",
       " 'and',\n",
       " 'secur',\n",
       " 'of',\n",
       " 'person',\n",
       " '.',\n",
       " 'articl',\n",
       " '4',\n",
       " 'no',\n",
       " 'one',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'held',\n",
       " 'in',\n",
       " 'slaveri',\n",
       " 'or',\n",
       " 'servitud',\n",
       " ';',\n",
       " 'slaveri',\n",
       " 'and',\n",
       " 'the',\n",
       " 'slave',\n",
       " 'trade',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'prohibit',\n",
       " 'in',\n",
       " 'all',\n",
       " 'their',\n",
       " 'form',\n",
       " '.',\n",
       " 'articl',\n",
       " '5',\n",
       " 'no',\n",
       " 'one',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'tortur',\n",
       " 'or',\n",
       " 'to',\n",
       " 'cruel',\n",
       " ',',\n",
       " 'inhuman',\n",
       " 'or',\n",
       " 'degrad',\n",
       " 'treatment',\n",
       " 'or',\n",
       " 'punish',\n",
       " '.',\n",
       " 'articl',\n",
       " '6',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'recognit',\n",
       " 'everywher',\n",
       " 'as',\n",
       " 'a',\n",
       " 'person',\n",
       " 'befor',\n",
       " 'the',\n",
       " 'law',\n",
       " '.',\n",
       " 'articl',\n",
       " '7',\n",
       " 'all',\n",
       " 'are',\n",
       " 'equal',\n",
       " 'befor',\n",
       " 'the',\n",
       " 'law',\n",
       " 'and',\n",
       " 'are',\n",
       " 'entitl',\n",
       " 'without',\n",
       " 'ani',\n",
       " 'discrimin',\n",
       " 'to',\n",
       " 'equal',\n",
       " 'protect',\n",
       " 'of',\n",
       " 'the',\n",
       " 'law',\n",
       " '.',\n",
       " 'all',\n",
       " 'are',\n",
       " 'entitl',\n",
       " 'to',\n",
       " 'equal',\n",
       " 'protect',\n",
       " 'against',\n",
       " 'ani',\n",
       " 'discrimin',\n",
       " 'in',\n",
       " 'violat',\n",
       " 'of',\n",
       " 'thi',\n",
       " 'declar',\n",
       " 'and',\n",
       " 'against',\n",
       " 'ani',\n",
       " 'incit',\n",
       " 'to',\n",
       " 'such',\n",
       " 'discrimin',\n",
       " '.',\n",
       " 'articl',\n",
       " '8',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'an',\n",
       " 'effect',\n",
       " 'remedi',\n",
       " 'by',\n",
       " 'the',\n",
       " 'compet',\n",
       " 'nation',\n",
       " 'tribun',\n",
       " 'for',\n",
       " 'act',\n",
       " 'violat',\n",
       " 'the',\n",
       " 'fundament',\n",
       " 'right',\n",
       " 'grant',\n",
       " 'him',\n",
       " 'by',\n",
       " 'the',\n",
       " 'constitut',\n",
       " 'or',\n",
       " 'by',\n",
       " 'law',\n",
       " '.',\n",
       " 'articl',\n",
       " '9',\n",
       " 'no',\n",
       " 'one',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'arbitrari',\n",
       " 'arrest',\n",
       " ',',\n",
       " 'detent',\n",
       " 'or',\n",
       " 'exil',\n",
       " '.',\n",
       " 'articl',\n",
       " '10',\n",
       " 'everyon',\n",
       " 'is',\n",
       " 'entitl',\n",
       " 'in',\n",
       " 'full',\n",
       " 'equal',\n",
       " 'to',\n",
       " 'a',\n",
       " 'fair',\n",
       " 'and',\n",
       " 'public',\n",
       " 'hear',\n",
       " 'by',\n",
       " 'an',\n",
       " 'independ',\n",
       " 'and',\n",
       " 'imparti',\n",
       " 'tribun',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'determin',\n",
       " 'of',\n",
       " 'hi',\n",
       " 'right',\n",
       " 'and',\n",
       " 'oblig',\n",
       " 'and',\n",
       " 'of',\n",
       " 'ani',\n",
       " 'crimin',\n",
       " 'charg',\n",
       " 'against',\n",
       " 'him',\n",
       " '.',\n",
       " 'articl',\n",
       " '11',\n",
       " 'everyon',\n",
       " 'charg',\n",
       " 'with',\n",
       " 'a',\n",
       " 'penal',\n",
       " 'offenc',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'be',\n",
       " 'presum',\n",
       " 'innoc',\n",
       " 'until',\n",
       " 'prove',\n",
       " 'guilti',\n",
       " 'accord',\n",
       " 'to',\n",
       " 'law',\n",
       " 'in',\n",
       " 'a',\n",
       " 'public',\n",
       " 'trial',\n",
       " 'at',\n",
       " 'which',\n",
       " 'he',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'all',\n",
       " 'the',\n",
       " 'guarante',\n",
       " 'necessari',\n",
       " 'for',\n",
       " 'hi',\n",
       " 'defenc',\n",
       " '.',\n",
       " 'no',\n",
       " 'one',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'held',\n",
       " 'guilti',\n",
       " 'of',\n",
       " 'ani',\n",
       " 'penal',\n",
       " 'offenc',\n",
       " 'on',\n",
       " 'account',\n",
       " 'of',\n",
       " 'ani',\n",
       " 'act',\n",
       " 'or',\n",
       " 'omiss',\n",
       " 'which',\n",
       " 'did',\n",
       " 'not',\n",
       " 'constitut',\n",
       " 'a',\n",
       " 'penal',\n",
       " 'offenc',\n",
       " ',',\n",
       " 'under',\n",
       " 'nation',\n",
       " 'or',\n",
       " 'intern',\n",
       " 'law',\n",
       " ',',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'when',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'commit',\n",
       " '.',\n",
       " 'nor',\n",
       " 'shall',\n",
       " 'a',\n",
       " 'heavier',\n",
       " 'penalti',\n",
       " 'be',\n",
       " 'impos',\n",
       " 'than',\n",
       " 'the',\n",
       " 'one',\n",
       " 'that',\n",
       " 'wa',\n",
       " 'applic',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'the',\n",
       " 'penal',\n",
       " 'offenc',\n",
       " 'wa',\n",
       " 'commit',\n",
       " '.',\n",
       " 'articl',\n",
       " '12',\n",
       " 'no',\n",
       " 'one',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'arbitrari',\n",
       " 'interfer',\n",
       " 'with',\n",
       " 'hi',\n",
       " 'privaci',\n",
       " ',',\n",
       " 'famili',\n",
       " ',',\n",
       " 'home',\n",
       " 'or',\n",
       " 'correspond',\n",
       " ',',\n",
       " 'nor',\n",
       " 'to',\n",
       " 'attack',\n",
       " 'upon',\n",
       " 'hi',\n",
       " 'honour',\n",
       " 'and',\n",
       " 'reput',\n",
       " '.',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'the',\n",
       " 'protect',\n",
       " 'of',\n",
       " 'the',\n",
       " 'law',\n",
       " 'against',\n",
       " 'such',\n",
       " 'interfer',\n",
       " 'or',\n",
       " 'attack',\n",
       " '.',\n",
       " 'articl',\n",
       " '13',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'movement',\n",
       " 'and',\n",
       " 'resid',\n",
       " 'within',\n",
       " 'the',\n",
       " 'border',\n",
       " 'of',\n",
       " 'each',\n",
       " 'state',\n",
       " '.',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'leav',\n",
       " 'ani',\n",
       " 'countri',\n",
       " ',',\n",
       " 'includ',\n",
       " 'hi',\n",
       " 'own',\n",
       " ',',\n",
       " 'and',\n",
       " 'to',\n",
       " 'return',\n",
       " 'to',\n",
       " 'hi',\n",
       " 'countri',\n",
       " '.',\n",
       " 'articl',\n",
       " '14',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'seek',\n",
       " 'and',\n",
       " 'to',\n",
       " 'enjoy',\n",
       " 'in',\n",
       " 'other',\n",
       " 'countri',\n",
       " 'asylum',\n",
       " 'from',\n",
       " 'persecut',\n",
       " '.',\n",
       " 'thi',\n",
       " 'right',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'invok',\n",
       " 'in',\n",
       " 'the',\n",
       " 'case',\n",
       " 'of',\n",
       " 'prosecut',\n",
       " 'genuin',\n",
       " 'aris',\n",
       " 'from',\n",
       " 'non',\n",
       " '-',\n",
       " 'polit',\n",
       " 'crime',\n",
       " 'or',\n",
       " 'from',\n",
       " 'act',\n",
       " 'contrari',\n",
       " 'to',\n",
       " 'the',\n",
       " 'purpos',\n",
       " 'and',\n",
       " 'principl',\n",
       " 'of',\n",
       " 'the',\n",
       " 'unit',\n",
       " 'nation',\n",
       " '.',\n",
       " 'articl',\n",
       " '15',\n",
       " 'everyon',\n",
       " 'ha',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'a',\n",
       " 'nation',\n",
       " '.',\n",
       " 'no',\n",
       " 'one',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'arbitrarili',\n",
       " 'depriv',\n",
       " 'of',\n",
       " 'hi',\n",
       " 'nation',\n",
       " 'nor',\n",
       " 'deni',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'chang',\n",
       " 'hi',\n",
       " 'nation',\n",
       " '.',\n",
       " 'articl',\n",
       " '16',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'of',\n",
       " 'full',\n",
       " 'age',\n",
       " ',',\n",
       " 'without',\n",
       " 'ani',\n",
       " 'limit',\n",
       " 'due',\n",
       " 'to',\n",
       " 'race',\n",
       " ',',\n",
       " 'nation',\n",
       " 'or',\n",
       " 'religion',\n",
       " ',',\n",
       " 'have',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'marri',\n",
       " 'and',\n",
       " 'to',\n",
       " 'found',\n",
       " 'a',\n",
       " 'famili',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'entitl',\n",
       " 'to',\n",
       " 'equal',\n",
       " 'right',\n",
       " 'as',\n",
       " 'to',\n",
       " 'marriag',\n",
       " ',',\n",
       " 'dure',\n",
       " 'marriag',\n",
       " 'and',\n",
       " 'at',\n",
       " 'it',\n",
       " 'dissolut',\n",
       " '.',\n",
       " 'marriag',\n",
       " 'shall',\n",
       " 'be',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in udhr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lemmatization: Stemming, but resulting stems are all valid words\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in udhr[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " 'not',\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = \"Children should not drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " 'not',\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how you would split sentences from a long text string\n",
    "text12 = \"This is the first sentence. A gallon of milk in the U.S costs $2.99. Is this this the third sentence. Yes, it is!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S costs $2.99.',\n",
       " 'Is this this the third sentence.',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sent_tokenizer\n",
    "sentences = nltk.sent_tokenize(text12)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance Natural Language Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.help.upenn_tagset('MD') ##Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text11 = \"Children should not drink a sugary drink before bed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text13 = nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk tokenizer getting the postags\n",
    "nltk.pos_tag(text13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ambiguity in POST Tagging\n",
    "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Visiting', 'VBG'),\n",
       " ('aunts', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('nuisance', 'NN')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'loves', 'Bob']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text15 = nltk.word_tokenize('Alice loves Bob')\n",
    "text15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Alice' | 'Bob'\n",
    "V -> 'loves'\n",
    "\"\"\")\n",
    "# there could be ambiguity even if the sentence is grammatically correct \n",
    "# we can write a grammar configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(text15)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nltk and Parse Tree Collection\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "text17 = treebank.parsed_sents('wsj_0001.mrg')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "print(text17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying features in text\n",
    "- words\n",
    "\n",
    "- stop-words (articules, commonly-occuring words)\n",
    "\n",
    "- normalization (make lower case vs. leave as-is)\n",
    "\n",
    "- stemming/lemmatization\n",
    "\n",
    "- characteristics of words: Capitalizacion\n",
    "\n",
    "- Parts of speech of words in sentences\n",
    "\n",
    "- Grammatical structure, sentences parcing\n",
    "\n",
    "- Grouping words of simmilar meaning, semantics (buy, purchase); (Mr. Dr. Prof.) (Numbers, Digits; Dates)\n",
    "\n",
    "- Bigrams, Trigrams, n-grams: \"White House\"\n",
    "\n",
    "- Character sub-sequences in words: \"ing\", \"ion\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Case of Study 3 classes.\n",
    "## Naive Bayes Classifier it is a probabilistic model, it is naive, \n",
    "# because it assumes features are independent of each other. given the class label\n",
    "# For classification problems, naive bayes provides very strong baselines.\n",
    "# it is a simple model, easy to learn parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two classic naive bayes variants for text\n",
    "- Multinomial Naive Bayes:\n",
    "\n",
    "    Data folows a multinomial distribution (word frequency is important)\n",
    "\n",
    "    Each feature value is a count (word ocurrenies counts, TF-IDF, weights) \n",
    "\n",
    "    The number of ocurrenies of the words is important\n",
    "    \n",
    "    So it is important to know the frequecy of the words\n",
    "\n",
    "- Bernoulli Naive Bayes: (word frequency is not important)\n",
    "\n",
    "    Data folows a miltinomial bernoulli distribution\n",
    "\n",
    "    Each feature is binary (it is presente or not)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "Classifier is a function on input data.\n",
    "\n",
    "Decision Boundaries: Is the shape or the surface which separates one class from another.\n",
    "\n",
    "Data overfitting: Decision boundry learned over training data doesnt generalize to test data.\n",
    "\n",
    "Linear boundaries: Easy to find, easy to evaluate\n",
    "\n",
    "\"Simple models generalize well\". Finding linear boundaries:\n",
    "- Perceptron, Linear Discriminative Analysis, Linear least squares\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Amazon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \\\n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2       5                                       Very pleased           0.0   \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0   \n",
       "\n",
       "   Positively Rated  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "## dropping al rows with null values\n",
    "df.dropna(inplace = True)\n",
    "## getting only the rows with rating is different than 3\n",
    "df = df[df['Rating']!=3]\n",
    "## creating the column Positively Rate \n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3,1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7504725897920604"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we got an imbalance class\n",
    "df['Positively Rated'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## Spliting in training and test sets\n",
    "X = df['Reviews']\n",
    "y = df['Positively Rated']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (3967,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '01',\n",
       " '02',\n",
       " '03pm',\n",
       " '04pm',\n",
       " '08',\n",
       " '0829',\n",
       " '0cant',\n",
       " '0ghz',\n",
       " '0mp',\n",
       " '0mps87ysgrvp9gkhps5d',\n",
       " '10',\n",
       " '100',\n",
       " '10000',\n",
       " '100gb',\n",
       " '101',\n",
       " '1020',\n",
       " '1080',\n",
       " '1080p',\n",
       " '10am',\n",
       " '10gb',\n",
       " '10mp',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '110v',\n",
       " '110w',\n",
       " '115',\n",
       " '11pm',\n",
       " '12',\n",
       " '120',\n",
       " '125',\n",
       " '128',\n",
       " '1280',\n",
       " '1280x720',\n",
       " '128gb',\n",
       " '128gigs',\n",
       " '12mpx',\n",
       " '13',\n",
       " '132',\n",
       " '13mp',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1400mah',\n",
       " '140g',\n",
       " '1440p',\n",
       " '149',\n",
       " '15',\n",
       " '150',\n",
       " '1520',\n",
       " '15ended',\n",
       " '15updated',\n",
       " '16',\n",
       " '160',\n",
       " '169',\n",
       " '16g',\n",
       " '16gb',\n",
       " '16mp',\n",
       " '17',\n",
       " '1700',\n",
       " '171730876692',\n",
       " '175',\n",
       " '179',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '1920x1080',\n",
       " '1920x1080p',\n",
       " '195',\n",
       " '196',\n",
       " '1970',\n",
       " '199',\n",
       " '1999',\n",
       " '19th',\n",
       " '1gb',\n",
       " '1hoir',\n",
       " '1mbps',\n",
       " '1min',\n",
       " '1month',\n",
       " '1mp',\n",
       " '1power',\n",
       " '1s',\n",
       " '1st',\n",
       " '1this',\n",
       " '1x',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2009',\n",
       " '200g',\n",
       " '200gb',\n",
       " '200mb',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2015battery',\n",
       " '2015okay',\n",
       " '2015phone',\n",
       " '2016',\n",
       " '2016just',\n",
       " '2017',\n",
       " '2025',\n",
       " '20781',\n",
       " '20g',\n",
       " '20ish',\n",
       " '20x',\n",
       " '21',\n",
       " '2100',\n",
       " '214',\n",
       " '2153843082',\n",
       " '22',\n",
       " '220',\n",
       " '220i',\n",
       " '235',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " '2407749011',\n",
       " '2407749011_p0_h',\n",
       " '249',\n",
       " '25',\n",
       " '250',\n",
       " '256',\n",
       " '256mb',\n",
       " '25cm',\n",
       " '260',\n",
       " '270',\n",
       " '277',\n",
       " '28',\n",
       " '280',\n",
       " '288',\n",
       " '29',\n",
       " '2910mah',\n",
       " '2915mah',\n",
       " '294',\n",
       " '2950',\n",
       " '299',\n",
       " '2amp',\n",
       " '2d',\n",
       " '2days',\n",
       " '2g',\n",
       " '2gb',\n",
       " '2ghz',\n",
       " '2in1',\n",
       " '2k',\n",
       " '2mm',\n",
       " '2mp',\n",
       " '2nd',\n",
       " '2pm',\n",
       " '2really',\n",
       " '2wks',\n",
       " '2x',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '3000mah',\n",
       " '302',\n",
       " '303',\n",
       " '30am',\n",
       " '30fps',\n",
       " '30ish',\n",
       " '30mb',\n",
       " '30pm',\n",
       " '30sec',\n",
       " '30th',\n",
       " '30x',\n",
       " '31',\n",
       " '312',\n",
       " '31st',\n",
       " '32',\n",
       " '324',\n",
       " '32gb',\n",
       " '330',\n",
       " '33831',\n",
       " '34',\n",
       " '35',\n",
       " '350',\n",
       " '3500mah',\n",
       " '360',\n",
       " '360p',\n",
       " '368',\n",
       " '36megapixels',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '39pm',\n",
       " '3am',\n",
       " '3d',\n",
       " '3g',\n",
       " '3gb',\n",
       " '3gcellphone',\n",
       " '3gs',\n",
       " '3rd',\n",
       " '3weeks',\n",
       " '3x',\n",
       " '3years',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '401',\n",
       " '401ppi',\n",
       " '405',\n",
       " '410',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '45am',\n",
       " '48',\n",
       " '480p',\n",
       " '480x854',\n",
       " '4core',\n",
       " '4g',\n",
       " '4gb',\n",
       " '4ghz',\n",
       " '4glte',\n",
       " '4in',\n",
       " '4k',\n",
       " '4months',\n",
       " '4pm',\n",
       " '4s',\n",
       " '4years',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50mb',\n",
       " '512mb',\n",
       " '52',\n",
       " '520',\n",
       " '54',\n",
       " '55',\n",
       " '55mb',\n",
       " '577',\n",
       " '59',\n",
       " '5c',\n",
       " '5g',\n",
       " '5gb',\n",
       " '5ghz',\n",
       " '5in',\n",
       " '5ish',\n",
       " '5j',\n",
       " '5mm',\n",
       " '5mp',\n",
       " '5s',\n",
       " '5th',\n",
       " '5x',\n",
       " '5xdesignthis',\n",
       " '60',\n",
       " '600',\n",
       " '60fps',\n",
       " '615',\n",
       " '62',\n",
       " '64',\n",
       " '640',\n",
       " '64g',\n",
       " '64gb',\n",
       " '650',\n",
       " '652',\n",
       " '66',\n",
       " '67',\n",
       " '6hrs',\n",
       " '6i',\n",
       " '6in',\n",
       " '6mb',\n",
       " '6p',\n",
       " '6s',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '700mhz',\n",
       " '70mb',\n",
       " '70s',\n",
       " '71',\n",
       " '710',\n",
       " '720',\n",
       " '720p',\n",
       " '740',\n",
       " '75',\n",
       " '750',\n",
       " '75gb',\n",
       " '77',\n",
       " '78',\n",
       " '7am',\n",
       " '7hr',\n",
       " '7inch',\n",
       " '80',\n",
       " '800',\n",
       " '801',\n",
       " '802',\n",
       " '805',\n",
       " '80gb',\n",
       " '81',\n",
       " '820',\n",
       " '830',\n",
       " '84',\n",
       " '85',\n",
       " '850',\n",
       " '855',\n",
       " '88',\n",
       " '8890',\n",
       " '8gb',\n",
       " '8inch',\n",
       " '8mp',\n",
       " '90',\n",
       " '900',\n",
       " '911',\n",
       " '92',\n",
       " '925',\n",
       " '93',\n",
       " '930',\n",
       " '935fd',\n",
       " '95',\n",
       " '98',\n",
       " '99',\n",
       " '99dollars',\n",
       " '9pm',\n",
       " '9th',\n",
       " '____',\n",
       " 'a1',\n",
       " 'a15',\n",
       " 'a3',\n",
       " 'a5',\n",
       " 'a53',\n",
       " 'a53s',\n",
       " 'a57',\n",
       " 'a7',\n",
       " 'a8',\n",
       " 'aaaaa',\n",
       " 'aarp',\n",
       " 'abbey',\n",
       " 'ability',\n",
       " 'abit',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutly',\n",
       " 'absorbing',\n",
       " 'absurdly',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abysmal',\n",
       " 'abysmaltap',\n",
       " 'ac',\n",
       " 'acatel',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'accesories',\n",
       " 'accesorios',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessing',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accommodate',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accustomed',\n",
       " 'acer',\n",
       " 'achieve',\n",
       " 'acivate',\n",
       " 'acknowledged',\n",
       " 'acount',\n",
       " 'acquisition',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activating',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actually',\n",
       " 'acurate',\n",
       " 'acurrate',\n",
       " 'ad',\n",
       " 'adaptar',\n",
       " 'adapter',\n",
       " 'adapters',\n",
       " 'adaptert',\n",
       " 'adaptive',\n",
       " 'adaptor',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'addtion',\n",
       " 'adecuado',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhesive',\n",
       " 'adicional',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjusts',\n",
       " 'admirably',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'adobe',\n",
       " 'adolescent',\n",
       " 'adore',\n",
       " 'adroid',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'advert',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisements',\n",
       " 'advertising',\n",
       " 'advertized',\n",
       " 'advice',\n",
       " 'adviced',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advising',\n",
       " 'aesthetic',\n",
       " 'aesthetics',\n",
       " 'affect',\n",
       " 'affects',\n",
       " 'affiliated',\n",
       " 'affiliation',\n",
       " 'afford',\n",
       " 'affordability',\n",
       " 'affordable',\n",
       " 'afghanistan',\n",
       " 'aforementioned',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'aggravation',\n",
       " 'agian',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agonizingly',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agressive',\n",
       " 'ahead',\n",
       " 'ahold',\n",
       " 'aida64',\n",
       " 'aids',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'airdrop',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'akg',\n",
       " 'al',\n",
       " 'alarm',\n",
       " 'alarming',\n",
       " 'alarms',\n",
       " 'albeit',\n",
       " 'alcatel',\n",
       " 'alcatels',\n",
       " 'alert',\n",
       " 'alexis',\n",
       " 'aligned',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allegedly',\n",
       " 'allevatev',\n",
       " 'alleviate',\n",
       " 'allot',\n",
       " 'allover',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allá',\n",
       " 'almoed',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alpha',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alternative',\n",
       " 'alternativei',\n",
       " 'alternatively',\n",
       " 'although',\n",
       " 'altogether',\n",
       " 'altough',\n",
       " 'aluminum',\n",
       " 'always',\n",
       " 'alwayspower',\n",
       " 'am',\n",
       " 'amazed',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'amazone',\n",
       " 'amazonhorrible',\n",
       " 'america',\n",
       " 'american',\n",
       " 'ammended',\n",
       " 'amoled',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoung',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amped',\n",
       " 'amzn',\n",
       " 'an',\n",
       " 'anal',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'andkeep',\n",
       " 'andoid',\n",
       " 'andriod',\n",
       " 'android',\n",
       " 'androidpicks',\n",
       " 'androids',\n",
       " 'ands',\n",
       " 'andthe',\n",
       " 'angle',\n",
       " 'angled',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'animation',\n",
       " 'animations',\n",
       " 'anime',\n",
       " 'announced',\n",
       " 'announcer',\n",
       " 'annoy',\n",
       " 'annoyances',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annoys',\n",
       " 'annually',\n",
       " 'anomalies',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'antartica',\n",
       " 'antenna',\n",
       " 'anti',\n",
       " 'anticipated',\n",
       " 'antiquated',\n",
       " 'antislip',\n",
       " 'antivirus',\n",
       " 'antivirusvery',\n",
       " 'antonio',\n",
       " 'antutu',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyonecan',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'aosp',\n",
       " 'ap',\n",
       " 'apaga',\n",
       " 'apart',\n",
       " 'apathetic',\n",
       " 'apecialy',\n",
       " 'aperture',\n",
       " 'apn',\n",
       " 'apologetic',\n",
       " 'apologized',\n",
       " 'app',\n",
       " 'appalled',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'applicaions',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appointments',\n",
       " 'appps',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciating',\n",
       " 'apprehension',\n",
       " 'apprehensive',\n",
       " 'approach',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approximately',\n",
       " 'apps',\n",
       " 'april',\n",
       " 'aps',\n",
       " 'apt',\n",
       " 'apx',\n",
       " 'aquarium',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arabic',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'argentina',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'arise',\n",
       " 'arived',\n",
       " 'arizona',\n",
       " 'arm',\n",
       " 'armor',\n",
       " 'armored',\n",
       " 'armorsuit',\n",
       " 'arms',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arranged',\n",
       " 'arrangement',\n",
       " 'arranging',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'art',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'articulo',\n",
       " 'artifacts',\n",
       " 'artificial',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'ascend',\n",
       " 'ascertain',\n",
       " 'ascree',\n",
       " 'asha',\n",
       " 'ashley',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'asides',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'asphalt',\n",
       " 'assembled',\n",
       " 'assembly',\n",
       " 'assertive',\n",
       " 'assess',\n",
       " 'assigned',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'assume',\n",
       " 'assumes',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'assured',\n",
       " 'assurred',\n",
       " 'asterisks',\n",
       " 'astonished',\n",
       " 'astonishing',\n",
       " 'astute',\n",
       " 'asurion',\n",
       " 'asus',\n",
       " 'at',\n",
       " 'ather',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'atrocious',\n",
       " 'att',\n",
       " 'attach',\n",
       " 'attached',\n",
       " 'attaches',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attempts',\n",
       " 'attention',\n",
       " 'attract',\n",
       " 'attractive',\n",
       " 'atvpdkikx0der',\n",
       " 'auction',\n",
       " 'audacity',\n",
       " 'audible',\n",
       " 'audio',\n",
       " 'audiobook',\n",
       " 'audiobooks',\n",
       " 'audiophile',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'aukey',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'austria',\n",
       " 'authentic',\n",
       " 'authority',\n",
       " 'authorized',\n",
       " 'autistic',\n",
       " 'auto',\n",
       " 'autofocus',\n",
       " 'automated',\n",
       " 'automatially',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'av',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'availablewhat',\n",
       " 'avant',\n",
       " 'average',\n",
       " 'average1',\n",
       " 'averaged',\n",
       " 'averting',\n",
       " 'avg',\n",
       " 'aviate',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'awarethis',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awesomw',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awsome',\n",
       " 'axon',\n",
       " 'b00pejqu9m',\n",
       " 'b00zoer95q',\n",
       " 'b01acxskag',\n",
       " 'b01auoracqand',\n",
       " 'b01cju9bbm',\n",
       " 'b01gyudmfy',\n",
       " 'b3',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babysitting',\n",
       " 'back',\n",
       " 'backed',\n",
       " 'backgammon',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'backing',\n",
       " 'backlight',\n",
       " 'backlit',\n",
       " 'backside',\n",
       " 'backspace',\n",
       " 'backup',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badbesides',\n",
       " 'badboth',\n",
       " 'badly',\n",
       " 'bads',\n",
       " 'badstill',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'bahasa',\n",
       " 'baisc',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balancing',\n",
       " 'balloon',\n",
       " 'bananas',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bandsstylish',\n",
       " 'bandwagon',\n",
       " 'bang',\n",
       " 'bank',\n",
       " 'bar',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'bars',\n",
       " 'base',\n",
       " 'based',\n",
       " 'baseline',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bassy',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'batches',\n",
       " 'batery',\n",
       " 'bath',\n",
       " 'batt',\n",
       " 'batter',\n",
       " 'batteries',\n",
       " 'battery',\n",
       " 'battery1',\n",
       " 'batteryif',\n",
       " 'batteryscreen',\n",
       " 'baytery',\n",
       " 'bb',\n",
       " 'bbb',\n",
       " 'bbq',\n",
       " 'bbrewer2',\n",
       " 'bc',\n",
       " 'bday',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beacouse',\n",
       " 'bear',\n",
       " 'bearings',\n",
       " 'beast',\n",
       " 'beastly',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beautifulcons',\n",
       " 'beautifully',\n",
       " 'beautify',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'becons',\n",
       " 'becuase',\n",
       " 'bed',\n",
       " 'bedore',\n",
       " 'beef',\n",
       " 'beefy',\n",
       " 'been',\n",
       " 'beep',\n",
       " 'beeps',\n",
       " 'beetle',\n",
       " 'before',\n",
       " 'began',\n",
       " 'beggining',\n",
       " 'begin',\n",
       " 'beginner',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behave',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beiber',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'bell',\n",
       " 'bells',\n",
       " 'belong',\n",
       " 'below',\n",
       " 'belt',\n",
       " 'bem',\n",
       " 'benchmarks',\n",
       " 'bend',\n",
       " 'beneath',\n",
       " 'beneficial',\n",
       " 'beneficiary',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'benjamin',\n",
       " 'bent',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bestsellers',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'beterrible',\n",
       " 'bettary',\n",
       " 'bettary7',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bezel',\n",
       " 'bezels',\n",
       " 'biased',\n",
       " 'bid',\n",
       " 'bien',\n",
       " 'big',\n",
       " 'bigeasy',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggies',\n",
       " 'biggy',\n",
       " 'bill',\n",
       " 'billed',\n",
       " 'bills',\n",
       " 'binge',\n",
       " 'birds',\n",
       " 'birght',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'bites',\n",
       " 'biting',\n",
       " 'bitterroot',\n",
       " 'bizarre',\n",
       " 'bl',\n",
       " 'bla',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blackberrys',\n",
       " 'blacked',\n",
       " 'blacklosted',\n",
       " 'blacks',\n",
       " 'blades',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blamed',\n",
       " 'blaming',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "## getting the vocabulary in trainin data\n",
    "vect.get_feature_names()[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7647"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3967x7647 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 126874 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tranformar a una matriz término documento\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9021627333789063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# aplicamos el vect.transform para crear la matriz término documento en testing\n",
    "y_pred = lr.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest coefs: ['work' 'terrible' 'broken' 'slow' 'lines' 'back' 'cracked' 'not' 'bad'\n",
      " 'junk']\n",
      "Largest coefs: ['expected' 'excelent' 'satisfied' 'good' 'best' 'excelente' 'perfect'\n",
      " 'awesome' 'excellent' 'love']\n"
     ]
    }
   ],
   "source": [
    "## Coefficients of the model\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort() ## genera una lista con las posiciones de los valores ordenados\n",
    "\n",
    "print('Smallest coefs:', feature_names[sorted_coef_index[:10]])\n",
    "print('Largest coefs:',feature_names[sorted_coef_index[-11:-1]])\n",
    "\n",
    "## Los coeficientes bajos corresponden a reviews negativas\n",
    "## Los coeficientes altos corresponden a reviews positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7542, 6747, 1142, 6196, 3965,  820, 1742, 4550,  834, 3747])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_coef_index[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf\n",
    "Allow us to weight words base on how important they are in a document.\n",
    "\n",
    "Es una forma de escalar los features.\n",
    "\n",
    "Se puede usar también para reducir la cantidad de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2422"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train) # features que aparecen solo en min 5 docs\n",
    "len(vect.get_feature_names()) # pasamos de 7k a 2k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC  0.8459432678377684\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "y_pred = lr.predict(X_test_vectorized)\n",
    "\n",
    "print('AUC ', roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest coefs: ['not' 'work' 'back' 'after' 'broken' 'terrible' 'slow' 'bad' 'returned'\n",
      " 'screen']\n",
      "Largest coefs: ['excelente' 'as' 'awesome' 'price' 'works' 'best' 'perfect' 'good'\n",
      " 'excellent' 'love']\n"
     ]
    }
   ],
   "source": [
    "## Coefficients of the model\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_tdidf_coef_index = lr.coef_[0].argsort()\n",
    "\n",
    "print('Smallest coefs:', feature_names[sorted_tdidf_coef_index[:10]])\n",
    "print('Largest coefs:',feature_names[sorted_tdidf_coef_index[-11:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43430906,  0.05715287,  0.60036299, ...,  0.33131396,\n",
       "        0.01863968, -0.11059223])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 3 elementos ordenados: [1 1 2]\n",
      "Ultimos 3 elementos ordenados: [ 5  6 11]\n"
     ]
    }
   ],
   "source": [
    "## Ejemplo de argsort\n",
    "a = np.array([4,5,6,3,1,2,11,33,2,1,4])\n",
    "sorted_a_index = a.argsort()\n",
    "\n",
    "print('Primeros 3 elementos ordenados:', a[sorted_a_index[:3]])\n",
    "print('Ultimos 3 elementos ordenados:', a[sorted_a_index[-4:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  9,  5,  8,  3,  0, 10,  1,  2,  6,  7])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_a_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8238"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range= (1,2))\n",
    "vect = vect.fit(X_train)\n",
    "len(vect.get_feature_names()) ## Pasamos de 7200 a 8200 featuress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.908126948090636\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred)) # Mejoramos un poquito de 0.902 a 0.908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest coefs: ['carry' 'for him' 'all in' 'accidentally' 'amount' 'exceptional'\n",
      " 'dropped it' 'all its' 'despite' 'disappointed']\n",
      "Largest coefs: ['aren' 'age' 'all good' 'constant' 'for in' 'alternative to' 'code'\n",
      " 'battery drains' 'argentina' 'but when']\n"
     ]
    }
   ],
   "source": [
    "## Coefficients of the model\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_tdidf_coef_index = lr.coef_[0].argsort()\n",
    "\n",
    "print('Smallest coefs:', feature_names[sorted_tdidf_coef_index[:10]])\n",
    "print('Largest coefs:',feature_names[sorted_tdidf_coef_index[-11:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Text Similarity\n",
    "- Grouping similar words into semantic concepts\n",
    "- Build block in natural language understanding task\n",
    "    - Textual entailment\n",
    "    - Paraphrasing\n",
    "- Word Net\n",
    "    - Semantic dictionary of words, interlinked by semantic relations\n",
    "    - It includes rich lingusitic information\n",
    "        - Part of speech, word senses, synonyms, hypernyms, ...\n",
    "    - Machine-readable, freely available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
